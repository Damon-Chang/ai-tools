{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI HW5: LLM Fine-tunning\n",
    "è¯¥ä»»åŠ¡åŒ…å«å¾®è°ƒä¸€ä¸ªå¤§è¯­è¨€æ¨¡å‹ä½¿å…¶èƒ½å¤Ÿå†™å”è¯—ã€‚\n",
    "**TODOs**\n",
    "1. é˜…è¯»ä½œä¸šå¹»ç¯ç‰‡ï¼Œç¡®ä¿ä½ ç†è§£è¯¥ä½œä¸šçš„ç›®æ ‡ã€‚\n",
    "2. ä¿å­˜ä¸€ä»½æœ¬ Colab ç¬”è®°æœ¬çš„å‰¯æœ¬ã€‚\n",
    "3. æŒ‰ç…§æœ¬ Colab ç¬”è®°æœ¬ä¸­çš„æ­¥éª¤å¾®è°ƒä½ çš„ LLMã€‚\n",
    "4. ä½¿ç”¨è¾¾å“¥æ‰¹æ”¹åŠ©æ•™è¯„ä¼°è¾“å‡º\n",
    "5. å°†ç»“æœæ›´æ–°åˆ° NTU COOL\n",
    "6. ï¼ˆå¯é€‰ï¼‰ä½¿ç”¨ [è¯„åˆ†è§£æç¨‹åº]() æ£€æŸ¥ä½ çš„åˆ†æ•°\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯åŠ¨GPU\n",
    "å› ä¸ºè¦å¾®è°ƒæ¨¡å‹ï¼Œå¿…é¡»å¯åŠ¨GPUï¼Œè¿™æ ·å·¥ä½œèƒ½å¤Ÿåœ¨åˆç†çš„æ—¶é—´ï¼ˆ1-2hï¼‰å†…å®Œæˆâœ…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers, datasets\n",
    "from peft import PeftModel\n",
    "from colorama import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import GenerationConfig\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¡«å…¥éšæœºç§å­\n",
    "å½“å¼•å…¥å¾®è°ƒè¿‡ç¨‹æ—¶ä¼šå¼•å…¥éšæœºæ€§ï¼Œå¡«å…¥éšæœºç§å­å¯ä»¥ä½¿ç»“æœå¯å¤åˆ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends\n",
    "'''\n",
    "è¿™æ®µä»£ç çš„ä½œç”¨æ˜¯è®¾ç½®éšæœºæ•°ç§å­ä»¥ç¡®ä¿å¯é‡å¤æ€§ã€‚\n",
    "é¦–å…ˆå°†å˜é‡ seed è®¾ç½®ä¸º 42ã€‚\n",
    "ç„¶åè®¾ç½® PyTorch çš„ cuDNN åç«¯ä¸ºç¡®å®šæ€§æ¨¡å¼å¹¶ä¸”å…³é—­åŸºå‡†æµ‹è¯•æ¨¡å¼ã€‚\n",
    "æ¥ç€ä½¿ç”¨ torch.manual_seed (seed) è®¾ç½® CPU ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚\n",
    "å¦‚æœæœ‰å¯ç”¨çš„ GPUï¼Œåˆ™ä½¿ç”¨ torch.cuda.manual_seed_all (seed) \n",
    "è®¾ç½®æ‰€æœ‰ GPU ä¸Šçš„éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚è¿™æ ·å¯ä»¥ä½¿å¾—åœ¨ä¸åŒçš„è¿è¡Œç¯å¢ƒä¸‹ï¼Œ\n",
    "ä»£ç çš„éšæœºæ€§ç»“æœä¿æŒä¸€è‡´ï¼Œä¾¿äºè°ƒè¯•å’Œå¤ç°å®éªŒç»“æœã€‚\n",
    "'''\n",
    "seed = 42\n",
    "torch.backends.cudnn.dereministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å®šä¹‰ä¸€äº›æœ‰ç”¨çš„å‡½æ•°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆè®­ç»ƒèµ„æ–™\n",
    "def generate_training_data(data_point):\n",
    "    \"\"\"\n",
    "    1.ç›®æ ‡\n",
    "        - è¯¥å‡½æ•°å°†æ•°æ®ï¼ˆè¾“å…¥å’Œè¾“å‡ºçš„æ–‡æœ¬ï¼‰è½¬æ¢æˆæ¨¡å‹å¯ä»¥è¯»æ‡‚çš„tokens\n",
    "\n",
    "    2.å‚æ•°\n",
    "        - data_point: è¾“å…¥çš„æ–‡æœ¬ï¼Œå­—å…¸ç±»å‹ï¼Œå­—æ®µ\"instruction\",\"input\",\"output\"éƒ½æ˜¯str\n",
    "\n",
    "    3.è¿”å›å€¼\n",
    "        - ä¸€ä¸ªåŒ…å«æ¨¡å‹è¾“å…¥æ ‡è®°ã€ä½¿æ¨¡å‹å…·æœ‰å› æœæ€§çš„æ³¨æ„åŠ›æ©ç ä»¥åŠç›¸åº”è¾“å‡ºç›®æ ‡çš„å­—å…¸\n",
    "    \"\"\"\n",
    "    # æ„é€ å®Œæ•´çš„è¾“å…¥prompt\n",
    "    prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹è€Œä¸”æ“…é•¿å†™å”è¯—ã€‚\n",
    "<</SYS>>\n",
    "\n",
    "{data_point[\"instruction\"]}\n",
    "{data_point[\"input\"]}\n",
    "\"\"\"\n",
    "    # è®¡æ•°è¾“å…¥çš„tokenæ•°é‡\n",
    "    # å³ç»Ÿè®¡ç»è¿‡åˆ†è¯å™¨åçš„\n",
    "    len_user_prompt_tokens = (\n",
    "        len(\n",
    "            tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=CUTOFF_LEN + 1,\n",
    "                padding=\"max_length\",\n",
    "            )[\"input_ids\"]\n",
    "        ) - 1\n",
    "    )\n",
    "    # å°†è¾“å…¥è½¬åŒ–ä¸ºtokens\n",
    "    full_tokens = tokenizer(\n",
    "        prompt + \" \" + data_point[\"output\"] + \"</s>\",\n",
    "        truncation=True, # å¦‚æœè¶…è¿‡æœ€å¤§é•¿åº¦ æˆªæ–­å¤„ç†\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"][:-1]\n",
    "    return {\n",
    "        \"input_ids\": full_tokens, # è¾“å…¥ä¸ºæ ·æœ¬çš„å…¨éƒ¨token\n",
    "        \"labels\": [-100] * len_user_prompt_tokens + \n",
    "        full_tokens[len_user_prompt_tokens:], # å‰åŠéƒ¨åˆ†-100è¡¨ç¤ºå¿½ç•¥ï¼ŒååŠéƒ¨åˆ†ä¸ºå®é™…è¾“å‡ºçš„tokens\n",
    "        \"attention_mask\" : [1] * (len(full_tokens)), # å…¨ä¸º1è¡¨ç¤ºæ‰€æœ‰ä½ç½®éƒ½å‚ä¸æ³¨æ„åŠ›æœºåˆ¶\n",
    "    }\n",
    "\n",
    "# è¿›è¡Œç”Ÿæˆå›å¤çš„è¯„ä¼°\n",
    "def evaluate(instruction, generation_config, max_len, input=\"\", verbose=True):\n",
    "    \"\"\"\n",
    "    1.ç›®æ ‡\n",
    "        - è¯¥å‡½æ•°ç”¨äºå¾—åˆ°ç»™å®šè¾“å…¥æ¨¡å‹çš„è¾“å‡º\n",
    "\n",
    "    2.å‚æ•°\n",
    "        - instruction: str, è®©æ¨¡å‹æƒ³åšä»€ä¹ˆçš„æè¿°\n",
    "        - generation_config: dict, æ¨¡å‹ç”Ÿæˆé…ç½®ï¼Œ\n",
    "          transformers.GenerationConfigç±»ç”¨äºæ§åˆ¶è¯­è¨€æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆè¿‡ç¨‹ã€‚\n",
    "          å®ƒå…è®¸ç”¨æˆ·æŒ‡å®šå„ç§å‚æ•°ï¼Œå¦‚ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ã€ç”Ÿæˆç­–ç•¥ã€æ¸©åº¦ï¼ˆtemperatureï¼‰è®¾ç½®ç­‰ï¼Œ\n",
    "          ä»è€Œæ ¹æ®å…·ä½“çš„éœ€æ±‚å’Œåœºæ™¯å®šåˆ¶æ–‡æœ¬ç”Ÿæˆçš„è¡Œä¸ºã€‚\n",
    "        - max_len: int, æ¨¡å‹è¾“å‡ºæœ€å¤§é•¿åº¦\n",
    "        - input: str, æ¨¡å‹è¾“å…¥çš„æ–‡æœ¬ï¼Œè¢«ç”¨äºè§£å†³instruction\n",
    "        - verbose: bool, æ˜¯å¦æ‰“å°è¾“å‡º\n",
    "    \n",
    "    3.è¿”å›å€¼\n",
    "        - output: str, æ ¹æ®è¾“å…¥å’Œæè¿°äº§ç”Ÿæ¨¡å‹å›å¤çš„æ¨¡å‹è¾“å‡ºçš„æ–‡æœ¬\n",
    "\n",
    "    \"\"\"\n",
    "    # æ„é€ å…¨éƒ¨prompt\n",
    "    prompt = f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹è€Œä¸”æ“…é•¿å†™å”è¯—ã€‚\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "{input}\n",
    "[/INST]\"\"\"\n",
    "    # å°†æç¤ºæ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€çš„æ•°å­—è¡¨ç¤º\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    #input_ids = inputs[\"input_ids\"].cuda() # å°†è¾“å…¥è½¬æ¢ä¸ºGPUä¸Šçš„å¼ é‡\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    # ä½¿ç”¨æ¨¡å‹è¿›è¡Œç”Ÿæˆå›å¤ ä½¿ç”¨çš„æ˜¯å¤§æ¨¡å‹çš„generateå‡½æ•°\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, # è¾“å…¥æ–‡æœ¬çš„ç¼–ç ï¼ˆæè¿°+æ–‡æœ¬ï¼‰\n",
    "        generation_config=generation_config, # ç”Ÿæˆé…ç½®\n",
    "        return_dict_in_generate=True, # è¿”å›å­—å…¸\n",
    "        output_scores=True, # è¿”å›åˆ†æ•°\n",
    "        max_new_tokens=max_len, # æœ€å¤§é•¿åº¦\n",
    "    )\n",
    "    # å°†ç”Ÿæˆçš„å›å¤è§£ç ï¼ˆæˆæ–‡å­—ï¼‰å¹¶æ‰“å°å‡º\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
    "        if (verbose):\n",
    "            print(output)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åœ¨å¾®è°ƒä¹‹å‰ä¸‹è½½æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†\n",
    "#### ä¸‹è½½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"../../chatglm3-6b/\" # æš‚æ—¶å…ˆç”¨æ™ºè°±çš„6bçš„å¤§æ¨¡å‹\n",
    "\n",
    "#model_name = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\" \n",
    "# é€‰æ‹© MediaTek Breeze 7B æ¨¡å‹ï¼Œå› ä¸º TAIDE æ¨¡å‹å¯èƒ½é‡åˆ°ä¸‹è½½æ¬¡æ•°è¿‡å¤šæ— æ³•è®¿é—®ã€‚\n",
    "# å®é™…ä¸Šæ¢å…¶ä»–çš„å¼€æºå¤§æ¨¡å‹ä¸€æ ·å¯ä»¥ï¼Œå¹¶ä¸å½±å“å­¦ä¹ ã€‚\n",
    "\n",
    "#model_name = \"/content/taide_7b\" # for colab\n",
    "model_name = \"../../taide_7b\" # è¦å¾®è°ƒçš„å¤§æ¨¡å‹åç§°\n",
    "# ç­‰å¾…æœªæ¥ä¸‹è½½æˆåŠŸå§\n",
    "#!wget -O taide_7b.zip \"https://www.dropbox.com/scl/fi/rlzdoidgejt9wiox88gz5/taide_7b.zip?rlkey=vtuc8xq7mq2d0dpuijxlb3jfj&dl=0\"\n",
    "#!unzip taide_7b.zip\n",
    "#os.listdir(\"../../chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export HF_ENDPOINT=https://hf-mirror.com\n",
    "#!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æ¨ç†\n",
    "æŸ¥çœ‹å¾®è°ƒä¹‹å‰æ¨¡å‹å®Œæˆå¾—æ€ä¹ˆæ ·"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ¨¡å‹åŠ è½½å‡ºç°äº†é—®é¢˜ï¼Œå¯èƒ½æ˜¯ä»¥ä¸‹çš„é—®é¢˜ä¹‹ä¸€: \n",
      "ï¼ˆ1ï¼‰åœ¨Macä¸ŠåŠ è½½æ¨¡å‹ï¼›\n",
      "ï¼ˆ2ï¼‰æ¨¡å‹åç§°é”™è¯¯ï¼›\n",
      "ï¼ˆ3ï¼‰å…¶ä»–è®¾ç½®é”™è¯¯.\n"
     ]
    }
   ],
   "source": [
    "'''å»ºè®®ï¼šä¸ä¿®æ”¹'''\n",
    "\n",
    "cache_dir = \"cache_file/fine_tuning_cache\"\n",
    "\n",
    "# æ¨¡å‹å‚æ•°è®¾ç½® åŠ è½½æ¨¡å‹çš„æ–¹å¼\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "try:\n",
    "    # ä»æŒ‡å®šçš„æ¨¡å‹åç§°æˆ–è·¯å¾„è½½å…¥é¢„è®­ç»ƒçš„è¯­è¨€æ¨¡å‹\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name, # æ¨¡å‹åç§°æˆ–è·¯å¾„ï¼Œè¿™é‡Œæ˜¯è·¯å¾„\n",
    "        cache_dir=cache_dir,\n",
    "        quantization_config=nf4_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "except:\n",
    "    print(\"æ¨¡å‹åŠ è½½å‡ºç°äº†é—®é¢˜ï¼Œå¯èƒ½æ˜¯ä»¥ä¸‹çš„é—®é¢˜ä¹‹ä¸€: \\nï¼ˆ1ï¼‰åœ¨Macä¸ŠåŠ è½½æ¨¡å‹ï¼›\\nï¼ˆ2ï¼‰æ¨¡å‹åç§°é”™è¯¯ï¼›\\nï¼ˆ3ï¼‰å…¶ä»–è®¾ç½®é”™è¯¯.\")\n",
    "# é‡åˆ°é”™è¯¯ æŸ¥äº†ä¸€ä¸‹å¤§æ¦‚ç‡æ˜¯macä¸Šé¢æ— æ³•ä»¥4bitæˆ–è€…8bitä¸ŠåŠ è½½æ¨¡å‹\n",
    "# å¤§æ¦‚ç‡éœ€è¦ç”¨åˆ°GPU å‘œå‘¼å‘¼ æ²¡æœ‰æœåŠ¡å™¨å¯¸æ­¥éš¾è¡Œå“¦\n",
    "\n",
    "\n",
    "# è®¾ç½®åä¸ºâ€œtransformerâ€çš„æ—¥å¿—è®°å½•å™¨çš„æ—¥å¿—çº§åˆ«ä¸º ERRORã€‚\n",
    "# è¿™æ„å‘³ç€åªæœ‰ä¸¥é‡é”™è¯¯çº§åˆ«åŠä»¥ä¸Šçš„æ—¥å¿—æ¶ˆæ¯ä¼šè¢«è®°å½•ä¸‹æ¥ï¼Œ\n",
    "# è€Œä½äº ERROR çº§åˆ«çš„æ—¥å¿—æ¶ˆæ¯ï¼ˆå¦‚ DEBUGã€INFO å’Œ WARNINGï¼‰å°†è¢«å¿½ç•¥ã€‚\n",
    "logging.getLogger(\"transformer\").setLevel(logging.ERROR) \n",
    "# åˆ›å»ºtokenizerå¹¶è®¾å®šç»“æŸç¬¦å·ï¼ˆeos_tokenï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,\n",
    "    cache_dir=cache_dir,\n",
    "    quantization_config=nf4_config,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# è®¾å®šæ¨¡å‹æ¨ç†æ—¶éœ€è¦ç”¨åˆ°çš„decoding patameters\n",
    "max_len = 128\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,         # ä»¥é‡‡æ ·çš„æ–¹å¼ç”Ÿæˆæ–‡æœ¬ é€‚åˆè¯—æ­Œ\n",
    "    tempture=0.1,           # çŸ¥è¯†é—®ç­”é‡‡ç”¨è¾ƒä½çš„æ¸©åº¦\n",
    "    num_beams=1,            # æŸæœç´¢å®½åº¦ \n",
    "    top_p=0.3,              # æ ¸é‡‡æ ·é˜ˆå€¼ é¿å…ä¸åˆç†çš„è¯æ±‡ å¹¶ä¿è¯æ–‡æœ¬çš„éšæœºæ€§\n",
    "    no_repeat_ngram_size=2, # ç¦æ­¢é‡å¤2-gram\n",
    "    pad_token_id=2,         # å¡«å……tokenå¯¹åº”çš„id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°è¯•ä¸åŠ é‚£äº›é™åˆ¶å‚æ•° ç«Ÿç„¶æˆåŠŸäº†\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿›è¡Œæ¨ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸å»ºè®®æ›´æ”¹\n",
    "\n",
    "# å°æ ·ç¤ºä¾‹\n",
    "test_tang_list = ['ç›¸è¦‹æ™‚é›£åˆ¥äº¦é›£ï¼Œæ±é¢¨ç„¡åŠ›ç™¾èŠ±æ®˜ã€‚', 'é‡å¸·æ·±ä¸‹è«æ„å ‚ï¼Œè‡¥å¾Œæ¸…å®µç´°ç´°é•·ã€‚', 'èŠ³è¾°è¿½é€¸è¶£ï¼Œç¦è‹‘ä¿¡å¤šå¥‡ã€‚']\n",
    "\n",
    "# ä¸ºæ¯ä¸ªæ ·æœ¬å¾—åˆ°æ¨¡å‹è¾“å‡º\n",
    "demo_before_fintune = []\n",
    "for tang in test_tang_list:\n",
    "    demo_before_fintune.append(f\"æ¨¡å‹è¾“å…¥:\\nä»¥ä¸‹æ˜¯ä¸€é¦–å”è¯—çš„ç¬¬ä¸€å¥è¯ï¼Œè¯·ç”¨ä½ çš„è®¤çŸ¥åˆ¤æ–­å¹¶å®Œæˆæ•´é¦–è¯—ã€‚{tang}\\n\\næ¨¡å‹è¾“å‡º:\\n\"\n",
    "                               +evaluate(\"ä»¥ä¸‹æ˜¯ä¸€é¦–å”è¯—çš„ç¬¬ä¸€å¥è¯ï¼Œè¯·ç”¨ä½ çš„è®¤çŸ¥åˆ¤æ–­å¹¶å®Œæˆæ•´é¦–è¯—ã€‚\",\n",
    "                                         generation_config=generation_config,\n",
    "                                         max_len=max_len,\n",
    "                                         input=tang,\n",
    "                                         verbose=False\n",
    "                                         ))\n",
    "    \n",
    "# æ‰“å°å¹¶å­˜å‚¨ç»“æœåˆ°æ–‡ä»¶ä¸­\n",
    "for idx in range(len(demo_before_fintune)):\n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(demo_before_fintune[idx])\n",
    "    print(\"_\" * 80)\n",
    "\n",
    "# 2025/1/8 22:40 æ¨ç†äº†30minä¹Ÿæ²¡æœ‰å‡ºç°ç»“æœ ç”¨æœ¬åœ°æ¨ç†è¿˜æ˜¯æ…¢å•Š\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è®¾ç½®å¾®è°ƒè¿‡ç¨‹çš„è¶…å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''å»ºè®®ä¿®æ”¹ä¸‹é¢çš„å‚æ•°'''\n",
    "num_train_data = 1040 # è®¾å®šç”¨äºè®­ç»ƒçš„èµ„æ–™æ•°é‡ï¼Œå¯è®¾ç½®çš„æœ€å¤§å€¼ä¸º5000ï¼Œåœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹å¸Œæœ›è®­ç»ƒèµ„æ–™è¶Šå¤šè¶Šå¥½\n",
    "                      # è¿™ä¼šè®©æ¨¡å‹çœ‹å¤å“¦æ›´å¤šæ ·åŒ–çš„è¯—å¥ï¼Œè¿›è€Œæå‡ç”Ÿæˆå“è´¨ï¼Œä½†ä¹Ÿä¼šå¢åŠ è®­ç»ƒçš„æ—¶é—´\n",
    "                      # ä½¿ç”¨é¢„è®¾å‚æ•°ï¼ˆ1040ï¼‰ï¼šfine-tuningå¤§è¶Šéœ€è¦25minï¼Œå®Œæ•´è·‘å®Œæ‰€æœ‰cellå¤§è¶Šéœ€è¦50min\n",
    "                      # ä½¿ç”¨æœ€å¤§å€¼ï¼ˆ5000ï¼‰ï¼šfine-tuningå¤§è¶Šéœ€è¦100minï¼Œå®Œæ•´è·‘å®Œæ‰€æœ‰cellå¤§è¶Šéœ€è¦2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''å¿…è¦æƒ…å†µä¸‹å¯ä»¥ä¿®æ”¹ä¸‹é¢çš„ä»£ç '''\n",
    "\n",
    "#output_dir = \"/content/drive/MyDrive\" # for colab\n",
    "output_dir = \"hm_save/hm5\"\n",
    "ckpt_dir = \"hm_save/hm5_ckpt\" # è®¾å®šmodel checkpointå­˜å‚¨ç›®å½•\n",
    "num_epoch = 1 # è®¾å®šè®­ç»ƒçš„æ€»Epochæ•°\n",
    "LEARNING_RATE = 3e-4 # å­¦ä¹ ç‡\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ä¸å»ºè®®ä¿®æ”¹ä¸‹ä¹°çš„ä»£ç '''\n",
    "\n",
    "cache_dir = \"cache_file/fine_tuning_cache\" # ç¼“å­˜ç›®å½•\n",
    "from_ckpt = False # æ˜¯å¦ä»checkpointè½½å…¥æ¨¡å‹çš„æƒé‡\n",
    "ckpt_name = None # æ¨¡å‹æƒé‡æ–‡ä»¶å\n",
    "dataset_dir = \"hm_dataset/GenAI-Hw5/Tang_training_data.json\"\n",
    "logging_steps = 20 # 20æ­¥è¾“å‡ºä¸€æ¬¡è®­ç»ƒæ—¥å¿—\n",
    "save_steps = 65 # å®šä¹‰è®­ç»ƒè¿‡ç¨‹ä¸­æ¯éš”å¤šå°‘æ­¥éª¤ä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
    "save_total_limit = 3 # æ§åˆ¶æœ€å¤šä¿ç•™å‡ ä¸ªæ¨¡å‹checkpoint\n",
    "report_to = None # è®¾å®šä¸ŠæŠ¥ä¸æŒ‡æ ‡çš„ç›®æ ‡ï¼Œé¢„è®¾ä¸ºæ— \n",
    "MICRO_BATCH_SIZE = 4 # å®šä¹‰å°æ‰¹æ¬¡çš„å¤§å°\n",
    "BATCH_SIZE = 16 # å®šä¹‰ä¸€ä¸ªæ‰¹æ¬¡çš„å¤§å°\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE # è®¡ç®—æ¯ä¸ªå°çš®åˆºç´¯ç§¯çš„æ¢¯åº¦å¸ƒç½²\n",
    "CUTOFF_LEN = 256 # è®¾å®šæ–‡æœ¬æˆªæ–­çš„æœ€å¤§é•¿åº¦\n",
    "\n",
    "# LoRAå¾®è°ƒå‚æ•°ï¼ˆæ­¤loraéå½¼loraï¼‰\n",
    "LORA_R = 8 # è®¾å®šLORAï¼ˆLayer-wise Random Attentionï¼‰çš„Rå€¼\n",
    "LORA_ALPHA = 16 # è®¾å®šLORAçš„alphaå€¼     \n",
    "LORA_DROPOUT = 0.05 # è®¾å®šLORAçš„dropoutå€¼\n",
    "VAL_SET_SIZE = 0 # è®¾å®šéªŒè¯é›†å¤§å° é¢„è®¾ä¸ºæ— \n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]# è®¾å®šç›®æ ‡è†œç»„ï¼Œè¿™äº›è†œç»„çš„æƒé‡å°†è¢«ä¿å­˜ä¸ºcheckpointæ–‡ä»¶\n",
    "\n",
    "device_map = \"auto\" # è®¾å®šè®¾å¤‡æ˜ å°„ï¼Œautoè¡¨ç¤ºè‡ªåŠ¨æ˜ å°„\n",
    "world_size= int(os.environ.get(\"WORLD_SIZE\", 1)) # è·å–ç¯å¢ƒå˜é‡WORLD_SIZEçš„å€¼ï¼Œå¦‚æœæ²¡æœ‰è®¾ç½®ï¼Œåˆ™é»˜è®¤ä¸º1\n",
    "ddp = world_size != 1 # æ ¹æ®world_sizeçš„å€¼åˆ¤æ–­æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼€å§‹å¾®è°ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddf0052227e475284b0301e7211bd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50824a38311447578158be82970bd75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# ä½¿ç”¨Transformer Trainerè¿›è¡Œæ¨¡å‹è®­ç»ƒ\\ntrainer = transformers.Trainer(\\n    model=model, # è¦è®­ç»ƒçš„æ¨¡å‹\\n    train_dataset=train_data,\\n    eval_dataset=val_data,\\n    args=transformers.TrainingArguments(\\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\\n        warmup_steps=50,\\n        num_train_epochs=num_epoch,\\n        learning_rate=LEARNING_RATE,\\n        #fp16=True, # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\\n        logging_steps=logging_steps,\\n        save_strategy=\"steps\",\\n        save_steps=save_steps,\\n        output_dir=ckpt_dir,\\n        save_total_limit=save_total_limit,\\n        ddp_find_unused_parameters=False if ddp else None, # æ˜¯å¦ä½¿ç”¨DDP æ§åˆ¶æ¢¯åº¦æ›´æ–°ç­–ç•¥\\n        report_to=report_to,\\n    ),\\n    # å¤„ç†è¾“å…¥æ•°æ®ï¼Œä¸è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡\\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n)\\n\\n# ä»…ç”¨æ¨¡å‹çš„cacheåŠŸèƒ½\\nmodel.config.use_cache = False\\n\\n# è‹¥ä½¿ç”¨ Pytorch 2.0 ä»¥ä¸Šç‰ˆæœ¬ä¸”é Windows ç³»ç»Ÿï¼Œ è¿›è¡Œæ¨¡å‹ç¼–è¯‘\\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\\n    model = torch.compile(model)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''ä¸å»ºè®®ä¿®æ”¹ä»¥ä¸‹ä»£ç '''\n",
    "\n",
    "# åˆ›é€ è¾“å‡ºè·¯å¾„\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# æ ¹æ®from_ckptæ ‡å¿—ï¼Œä»checkpointè½½å…¥æ¨¡å‹æƒé‡\n",
    "if from_ckpt:\n",
    "    print('Loading from checkpoint:', from_ckpt)\n",
    "    model = PeftModel.from_pretrained(model, ckpt_name)\n",
    "\n",
    "# å°†æ¨¡å‹å‡†å¤‡å¥½ä»¥ä½¿ç”¨INT8è®­ç»ƒ\n",
    "# è¿™é‡Œä¼°è®¡åœ¨macä¸Šä¹Ÿä¸ä¼šè¡Œå¾—é€š\n",
    "config = LoraConfig(\n",
    "    r=LORA_R, # LORA rank           # LoRAçš„ç§© æ§åˆ¶ä½ç§©çŸ©é˜µçš„ç»´åº¦\n",
    "    lora_alpha=LORA_ALPHA,          # å­¦ä¹ ç‡å› å­ å½±å“LoRAå±‚çš„å­¦ä¹ é€Ÿç‡\n",
    "    target_modules=TARGET_MODULES,  # æŒ‡å®šéœ€è¦åº”ç”¨çš„LoRAæ¨¡å—åˆ—è¡¨\n",
    "    lora_dropout=LORA_DROPOUT,      # è®¾ç½®LoRAå±‚ä¸­çš„dropoutæ¦‚ç‡\n",
    "    bias=\"none\",                    # ä¸ä½¿ç”¨åæ‰§é¡¹\n",
    "    task_type=\"CAUSAL_LM\"           # æŒ‡å®šä»»åŠ¡ç±»å‹ä¸ºå› æœè¯­è¨€æ¨¡å‹\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# å°†tokenizerçš„padding tokenè®¾å®šä¸º0\n",
    "# pad_token_id æ˜¯ç”¨äºå¡«å……åºåˆ—çš„ç‰¹æ®Šæ ‡è®° ID\n",
    "# é€šå¸¸åœ¨å¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥åºåˆ—æ—¶ä½¿ç”¨ï¼Œä»¥ç¡®ä¿æ‰€æœ‰åºåˆ—å…·æœ‰ç›¸åŒçš„é•¿åº¦\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# åŠ è½½å¹¶å¤„ç†è®­ç»ƒæ•°æ®\n",
    "with open(dataset_dir, \"r\", encoding = \"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "tmp_dataset = \"hm_dataset/GenAI-Hw5/tmp_dataset.json\"\n",
    "with open(tmp_dataset, \"w\", encoding = \"utf-8\") as f:\n",
    "    json.dump(data_json[:num_train_data], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "data = load_dataset(\n",
    "    \"json\", \n",
    "    data_files=tmp_dataset, \n",
    "    download_mode=\"force_redownload\"\n",
    "    )\n",
    "\n",
    "# å°†è®­ç»ƒæ•°æ®åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "if VAL_SET_SIZE > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=VAL_SET_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "    )\n",
    "    train_data = train_val[\"train\"].shuffle().map(generate_training_data)\n",
    "    val_data = train_val[\"test\"].shuffle().map(generate_training_data)\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_training_data)\n",
    "    val_data = None\n",
    "#'''\n",
    "# ä½¿ç”¨Transformer Trainerè¿›è¡Œæ¨¡å‹è®­ç»ƒ\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, # è¦è®­ç»ƒçš„æ¨¡å‹\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=50,\n",
    "        num_train_epochs=num_epoch,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        #fp16=True, # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ\n",
    "        logging_steps=logging_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        output_dir=ckpt_dir,\n",
    "        save_total_limit=save_total_limit,\n",
    "        ddp_find_unused_parameters=False if ddp else None, # æ˜¯å¦ä½¿ç”¨DDP æ§åˆ¶æ¢¯åº¦æ›´æ–°ç­–ç•¥\n",
    "        report_to=report_to,\n",
    "    ),\n",
    "    # å¤„ç†è¾“å…¥æ•°æ®ï¼Œä¸è¿›è¡Œæ©ç è¯­è¨€å»ºæ¨¡\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# ä»…ç”¨æ¨¡å‹çš„cacheåŠŸèƒ½\n",
    "model.config.use_cache = False\n",
    "\n",
    "# è‹¥ä½¿ç”¨ Pytorch 2.0 ä»¥ä¸Šç‰ˆæœ¬ä¸”é Windows ç³»ç»Ÿï¼Œ è¿›è¡Œæ¨¡å‹ç¼–è¯‘\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### å¼€å§‹è®­ç»ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
    "trainer.train()\n",
    "\n",
    "# å°†è®­ç»ƒå®Œçš„æ¨¡å‹ä¿å­˜åˆ°æŒ‡å®šçš„ç›®å½•ä¸­\n",
    "model.save_pretrained(ckpt_dir)\n",
    "\n",
    "# æ‰“å°è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ç¼ºå¤±æƒé‡çš„è­¦å‘Šä¿¡æ¯\n",
    "print(\"\\n å¦‚æœä¸Šé¢æœ‰å…³äºä¸¢å¤±å…³é”®è¯çš„è­¦å‘Šï¼Œè¯·å¿½ç•¥ :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æµ‹è¯•\n",
    "\n",
    "é¦–å…ˆåŠ è½½å¾®è°ƒæ¨¡å‹ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰€æœ‰å¯ç”¨checkpoint: \n",
      " id: checkpoint name\n"
     ]
    }
   ],
   "source": [
    "'''ä¸å»ºè®®ä¿®æ”¹ä¸‹é¢ä»£ç '''\n",
    "\n",
    "# æ‰¾åˆ°æ‰€æœ‰å¯ç”¨checkpoints\n",
    "ckpts = []\n",
    "for ckpt in os.listdir(ckpt_dir):\n",
    "    if (ckpt.startswith(\"checkpoint-\")):\n",
    "        ckpts.append(ckpt)\n",
    "\n",
    "# ç½—åˆ—æ‰€æœ‰çš„checkpoints\n",
    "ckpts = sorted(ckpts, key=lambda ckpt: int(ckpt.split(\"-\")[-1]))\n",
    "print(\"æ‰€æœ‰å¯ç”¨checkpoint: \")\n",
    "print(\" id: checkpoint name\")\n",
    "for (i, ckpt) in enumerate(ckpts):\n",
    "    print(f\"{i:>3} : {ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ä¸‹é¢çš„ä»£ç å¯ä»¥ä½†æ˜¯æ²¡å¿…è¦æ›´æ”¹'''\n",
    "\n",
    "id_of_ckpt_to_use = -1 # è¦ç”¨æ¥è¿›è¡Œæ¨ç†çš„checkpointçš„idï¼ˆå¯¹åº”ä¸Šä¸€ä¸ªcellè¾“å‡ºçš„ç»“æœï¼‰\n",
    "                       # é¢„è®¾å€¼-1æŒ‡çš„æ˜¯ä¸Šåˆ—checkpointsä¸­å€’æ•°ç¬¬ä¸€ä¸ª ä¹Ÿå°±æ˜¯æœ€åä¸€ä¸ªcheckpoint\n",
    "                       # å¦‚æœæƒ³è¦é€‰æ‹©å…¶ä»–checkpoint å¯ä»¥æŠŠ-1æ”¹æˆå…¶ä»–çš„æ•°å­—\n",
    "\n",
    "ckpt_name = os.path.join(ckpt_dir, ckpts[id_of_ckpt_to_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ä¸‹é¢çš„ä»£ç å¯ä»¥æ›´æ”¹ä½†æ˜¯æ²¡å¿…è¦'''\n",
    "# å¯ä»¥åœ¨è¿™é‡Œè°ƒæ•´encoding parameterï¼Œdecoding parameter\n",
    "max_len = 128 # ç”Ÿæˆå›å¤çš„æœ€å¤§é•¿åº¦\n",
    "temperature = 0.1 # è®¾å®šç”Ÿæˆå›å¤çš„éšæœºåº¦ å€¼è¶Šå°ç”Ÿæˆçš„å›å¤è¶Šç¨³å®š\n",
    "top_p = 0.3 # Top-p(nucleus sampling)æŠ½æ ·çš„æ¦‚ç‡é˜ˆå€¼ ç”¨äºæ§åˆ¶ç”Ÿæˆå›å¤çš„å¤šæ ·æ€§\n",
    "#top_k = 3 # è°ƒæ•´Top-kå€¼ ä»¥å¢åŠ ç”Ÿæˆå›å¤çš„å¤šæ ·æ€§å’Œé¿å…ç”Ÿæˆé‡å¤çš„è¯æ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ä¸æ¨èä¸‹é¢çš„ä»£ç '''\n",
    "\n",
    "test_data_path = \"hm_dataset/GenAI-Hw5/Tang_testing_data.json\"\n",
    "output_path = os.path.join(output_dir, \"results.txt\")\n",
    "\n",
    "cache_dir = \"cache_dir\\fine_tuing_cache\" # è®¾å®šå¿«å–ç›®å½•è·¯å¾„\n",
    "seed = 42 # è®¾å®šéšæœºç§å­ ç”¨äºé‡ç°ç»“æœ\n",
    "no_repeat_ngram_size = 3 # è®¾å®šç¦æ­¢é‡å¤Ngramçš„å¤§å° ç”¨äºé¿å…ç”Ÿæˆé‡å¤ç‰‡æ®µ\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # å¯ç”¨å››ä½é‡åŒ–\n",
    "    bnb_4bit_quant_type=\"nf4\", # æŒ‡å®šé‡åŒ–ç±»å‹ä¸ºnf4\n",
    "    bnb_4bit_use_double_quant=True, # å¯ç”¨åŒé‡é‡åŒ–\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # è®¾ç½®è®¡ç®—æ•°æ®ç±»å‹ä¸ºbfloat16\n",
    ")\n",
    "# è¿™äº›é…ç½®é€šå¸¸ç”¨äºä¼˜åŒ–æ¨¡å‹åœ¨ä½ç²¾åº¦ç¡¬ä»¶ä¸Šçš„æ€§èƒ½å’Œå†…å­˜ä½¿ç”¨\n",
    "\n",
    "# ä½¿ç”¨tokenizerå°†æ¨¡å‹åç§°è½¬æ¢æˆæ¨¡å‹å¯è¯†çš„æ•°ç»„è¡¨ç¤ºå½¢å¼\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name, # æŒ‡å®šæ¨¡å‹åç§°\n",
    "    cache_dir=cache_dir, # æŒ‡å®šç¼“å­˜ç›®å½•\n",
    "    quantization_config=nf4_config, # åº”ç”¨é‡åŒ–é…ç½®\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "# ä»é¢„è®­ç»ƒæ¨¡å‹è½½å…¥æ¨¡å‹å¹¶è®¾å®šä¸º8ä½æ•´æ•°ï¼ˆINT8ï¼‰æ¨¡å‹\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map={\"\":0}, # è®¾å®šä½¿ç”¨çš„è®¾å¤‡ æ­¤å¤„æŒ‡å®šä¸ºGPU0\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# ä»æŒ‡å®šçš„checkpointæ—©å…¥æ¨¡å‹æƒé‡\n",
    "model = PeftModel.from_pretrained(model, ckpt_name, device_map={\"\":0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''ä¸å»ºè®®ä¿®æ”¹ä¸‹é¢çš„ä»£ç '''\n",
    "\n",
    "results = []\n",
    "\n",
    "# è®¾å®šç”Ÿæˆé…ç½® åŒ…æ‹¬éšæœºåº¦ æŸæœç´¢ç­‰ç›¸å…³å‚æ•°\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    num_beams=1,\n",
    "    top_p=top_p,\n",
    "    #top_k = top_k,\n",
    "    no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "    pad_token_id=2,\n",
    ")\n",
    "\n",
    "# è¯»å–æµ‹è¯•èµ„æ–™\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# å¯¹äºæ¯ä¸ªæµ‹è¯•èµ„æ–™è¿›è¡Œé¢„æµ‹ å¹¶å­˜ä¸‹ç»“æœ\n",
    "#'''\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for (i, test_data) in enumerate(test_data):\n",
    "        predict = evaluate(instruction=test_data[\"instruction\"], \n",
    "                           generation_config=generation_config,\n",
    "                           max_len=max_len,\n",
    "                           input=test_data[\"input\"],\n",
    "                           verbose=False)\n",
    "        f.write(f\"{i+1}. \" + test_data[\"input\"] + predict + \"\\n\")\n",
    "        print(f\"{i+1}. \" + test_data[\"input\"] + predict)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æŸ¥çœ‹å¾®è°ƒæ¨¡å‹å¦‚ä½•æ¯”å¾—ä¸Šæ²¡æœ‰å¾®è°ƒçš„æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åƒä¹‹å‰ä¸€æ ·ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•ä¾‹å­\n",
    "test_tang_list = ['ç›¸è¦‹æ™‚é›£åˆ¥äº¦é›£ï¼Œæ±é¢¨ç„¡åŠ›ç™¾èŠ±æ®˜ã€‚', 'é‡å¸·æ·±ä¸‹è«æ„å ‚ï¼Œè‡¥å¾Œæ¸…å®µç´°ç´°é•·ã€‚', 'èŠ³è¾°è¿½é€¸è¶£ï¼Œç¦è‹‘ä¿¡å¤šå¥‡ã€‚']\n",
    "\n",
    "# åœ¨æ¨¡å‹å¾®è°ƒä¹‹å‰æ¨ç†\n",
    "demo_after_finetue = []\n",
    "for tang in test_tang_list:\n",
    "    demo_after_finetue.append(f\"æ¨¡å‹è¾“å…¥:\\nä»¥ä¸‹æ˜¯ä¸€é¦–å”è¯—çš„ç¬¬ä¸€å¥ï¼Œè¯·ç”¨ä½ çš„çŸ¥è¯†åˆ¤æ–­å¹¶å®Œæˆæ•´é¦–è¯—ã€‚{tang}\\\n",
    "                              \\n\\næ¨¡å‹è¾“å‡º:\\n\"+evaluate(instruction=\"ä»¥ä¸‹æ˜¯ä¸€é¦–å”è¯—çš„ç¬¬ä¸€å¥ï¼Œè¯·ç”¨ä½ çš„çŸ¥è¯†åˆ¤æ–­å¹¶å®Œæˆæ•´é¦–è¯—\",\n",
    "                                                       generation_config=generation_config,\n",
    "                                                       max_len=max_len,\n",
    "                                                       input=tang,\n",
    "                                                       verbose=False))\n",
    "    \n",
    "    # æ‰“å°å¹¶å­˜å‚¨ç»“æœåˆ°æ–‡æœ¬æ–‡ä»¶ä¸­\n",
    "    for idx in range(len(demo_after_finetue)):\n",
    "        print(f\"Example {idx + 1}:\")\n",
    "        print(demo_after_finetue[idx])\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### åˆ†å‰²çº¿\n",
    "ä¸‹é¢æ˜¯æˆ‘è‡ªå·±çš„æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"hm_dataset/GenAI-Hw5/Tang_testing_data.json\"\n",
    "output_path = os.path.join(output_dir, \"results.txt\")\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'ä»¥ä¸‹æ˜¯ä¸€é¦–å”è©©çš„ç¬¬ä¸€å¥è©±ï¼Œè«‹ç”¨ä½ çš„çŸ¥è­˜åˆ¤æ–·ä¸¦å®Œæˆæ•´é¦–è©©ã€‚', 'input': 'é›ªéœ½éŠ€å¦ç´ ï¼Œæ¡”é«˜æ˜ ç“Šæã€‚'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''åœ¨Macä¸Šè¿›è¡Œæ¨ç†çš„ç®€å•å‡½æ•°'''\n",
    "\n",
    "def easy_evaluate(instruction, max_len, input=\"\", verbose=True):\n",
    "    # construct full input prompt\n",
    "    prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€å€‹æ¨‚æ–¼åŠ©äººçš„åŠ©æ‰‹ä¸”æ“…é•·å¯«å”è©©ã€‚\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "{input}\n",
    "[/INST]\"\"\"\n",
    "    # å°‡æç¤ºæ–‡æœ¬è½‰æ›ç‚ºæ¨¡å‹æ‰€éœ€çš„æ•¸å­—è¡¨ç¤ºå½¢å¼\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    # ä½¿ç”¨æ¨¡å‹é€²è¡Œç”Ÿæˆå›è¦†\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        #generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_len,\n",
    "    )\n",
    "    # å°‡ç”Ÿæˆçš„å›è¦†è§£ç¢¼ä¸¦å°å‡º\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"<s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
    "        if (verbose):\n",
    "            print(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "output = easy_evaluate(instruction=test_data[0][\"instruction\"],\n",
    "                       max_len=max_len,\n",
    "                       input=test_data[0][\"input\"],\n",
    "                       verbose=True)\n",
    "# æ¨ç†é€Ÿåº¦æ€ä¹ˆè¿™ä¹ˆæ…¢å“ª äºŒåå¤šåˆ†é’Ÿéƒ½æ²¡æ¨ç†å®Œ éš¾é“çœŸçš„æ˜¯GPUæ‰å¯ä»¥å—ï¼ŸğŸ˜­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"../../taide_7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 56064\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>>\\nYou are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹è€Œä¸”æ“…é•¿å†™å”è¯—ã€‚\\n<</SYS>>\\n\\nä»¥ä¸‹æ˜¯ä¸€é¦–å”è©©çš„ç¬¬ä¸€å¥è©±ï¼Œè«‹ç”¨ä½ çš„çŸ¥è­˜åˆ¤æ–·ä¸¦å®Œæˆæ•´é¦–è©©ã€‚\\nç§¦å·é›„å¸å®…ï¼Œå‡½è°·å£¯çš‡å±…ã€‚\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dataset_dir, \"r\", encoding = \"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "    \n",
    "prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹è€Œä¸”æ“…é•¿å†™å”è¯—ã€‚\n",
    "<</SYS>>\n",
    "\n",
    "{data_json[0][\"instruction\"]}\n",
    "{data_json[0][\"input\"]}\n",
    "\"\"\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = tokenizer(\n",
    "    prompt,\n",
    "    truncation=True,\n",
    "    max_length=CUTOFF_LEN + 1,\n",
    "    padding=\"max_length\",\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>\\nYou are a helpful assistant and good at writing Tang poem. ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹è€Œä¸”æ“…é•¿å†™å”è¯—ã€‚\\n<</SYS>>\\n\\nä»¥ä¸‹æ˜¯ä¸€é¦–å”è©©çš„ç¬¬ä¸€å¥è©±ï¼Œè«‹ç”¨ä½ çš„çŸ¥è­˜åˆ¤æ–·ä¸¦å®Œæˆæ•´é¦–è©©ã€‚\\nç§¦å·é›„å¸å®…ï¼Œå‡½è°·å£¯çš‡å±…ã€‚\\n</s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
