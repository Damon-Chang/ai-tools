{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenAI HW5: LLM Fine-tunning\n",
    "该任务包含微调一个大语言模型使其能够写唐诗。\n",
    "**TODOs**\n",
    "1. 阅读作业幻灯片，确保你理解该作业的目标。\n",
    "2. 保存一份本 Colab 笔记本的副本。\n",
    "3. 按照本 Colab 笔记本中的步骤微调你的 LLM。\n",
    "4. 使用达哥批改助教评估输出\n",
    "5. 将结果更新到 NTU COOL\n",
    "6. （可选）使用 [评分解析程序]() 检查你的分数\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启动GPU\n",
    "因为要微调模型，必须启动GPU，这样工作能够在合理的时间（1-2h）内完成✅。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import json\n",
    "import warnings\n",
    "import logging\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import bitsandbytes as bnb\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import transformers, datasets\n",
    "from peft import PeftModel\n",
    "from colorama import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import GenerationConfig\n",
    "from peft import (\n",
    "    prepare_model_for_int8_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_kbit_training,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 填入随机种子\n",
    "当引入微调过程时会引入随机性，填入随机种子可以使结果可复制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends\n",
    "'''\n",
    "这段代码的作用是设置随机数种子以确保可重复性。\n",
    "首先将变量 seed 设置为 42。\n",
    "然后设置 PyTorch 的 cuDNN 后端为确定性模式并且关闭基准测试模式。\n",
    "接着使用 torch.manual_seed (seed) 设置 CPU 上的随机数生成器的种子。\n",
    "如果有可用的 GPU，则使用 torch.cuda.manual_seed_all (seed) \n",
    "设置所有 GPU 上的随机数生成器的种子。这样可以使得在不同的运行环境下，\n",
    "代码的随机性结果保持一致，便于调试和复现实验结果。\n",
    "'''\n",
    "seed = 42\n",
    "torch.backends.cudnn.dereministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义一些有用的函数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成训练资料\n",
    "def generate_training_data(data_point):\n",
    "    \"\"\"\n",
    "    1.目标\n",
    "        - 该函数将数据（输入和输出的文本）转换成模型可以读懂的tokens\n",
    "\n",
    "    2.参数\n",
    "        - data_point: 输入的文本，字典类型，字段\"instruction\",\"input\",\"output\"都是str\n",
    "\n",
    "    3.返回值\n",
    "        - 一个包含模型输入标记、使模型具有因果性的注意力掩码以及相应输出目标的字典\n",
    "    \"\"\"\n",
    "    # 构造完整的输入prompt\n",
    "    prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. 你是一个乐于助人的助手而且擅长写唐诗。\n",
    "<</SYS>>\n",
    "\n",
    "{data_point[\"instruction\"]}\n",
    "{data_point[\"input\"]}\n",
    "\"\"\"\n",
    "    # 计数输入的token数量\n",
    "    # 即统计经过分词器后的\n",
    "    len_user_prompt_tokens = (\n",
    "        len(\n",
    "            tokenizer(\n",
    "                prompt,\n",
    "                truncation=True,\n",
    "                max_length=CUTOFF_LEN + 1,\n",
    "                padding=\"max_length\",\n",
    "            )[\"input_ids\"]\n",
    "        ) - 1\n",
    "    )\n",
    "    # 将输入转化为tokens\n",
    "    full_tokens = tokenizer(\n",
    "        prompt + \" \" + data_point[\"output\"] + \"</s>\",\n",
    "        truncation=True, # 如果超过最大长度 截断处理\n",
    "        max_length=CUTOFF_LEN + 1,\n",
    "        padding=\"max_length\",\n",
    "    )[\"input_ids\"][:-1]\n",
    "    return {\n",
    "        \"input_ids\": full_tokens, # 输入为样本的全部token\n",
    "        \"labels\": [-100] * len_user_prompt_tokens + \n",
    "        full_tokens[len_user_prompt_tokens:], # 前半部分-100表示忽略，后半部分为实际输出的tokens\n",
    "        \"attention_mask\" : [1] * (len(full_tokens)), # 全为1表示所有位置都参与注意力机制\n",
    "    }\n",
    "\n",
    "# 进行生成回复的评估\n",
    "def evaluate(instruction, generation_config, max_len, input=\"\", verbose=True):\n",
    "    \"\"\"\n",
    "    1.目标\n",
    "        - 该函数用于得到给定输入模型的输出\n",
    "\n",
    "    2.参数\n",
    "        - instruction: str, 让模型想做什么的描述\n",
    "        - generation_config: dict, 模型生成配置，\n",
    "          transformers.GenerationConfig类用于控制语言模型的文本生成过程。\n",
    "          它允许用户指定各种参数，如生成的文本长度、生成策略、温度（temperature）设置等，\n",
    "          从而根据具体的需求和场景定制文本生成的行为。\n",
    "        - max_len: int, 模型输出最大长度\n",
    "        - input: str, 模型输入的文本，被用于解决instruction\n",
    "        - verbose: bool, 是否打印输出\n",
    "    \n",
    "    3.返回值\n",
    "        - output: str, 根据输入和描述产生模型回复的模型输出的文本\n",
    "\n",
    "    \"\"\"\n",
    "    # 构造全部prompt\n",
    "    prompt = f\"\"\"\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. 你是一个乐于助人的助手而且擅长写唐诗。\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "{input}\n",
    "[/INST]\"\"\"\n",
    "    # 将提示文本转换为模型所需的数字表示\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    #input_ids = inputs[\"input_ids\"].cuda() # 将输入转换为GPU上的张量\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    # 使用模型进行生成回复 使用的是大模型的generate函数\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids, # 输入文本的编码（描述+文本）\n",
    "        generation_config=generation_config, # 生成配置\n",
    "        return_dict_in_generate=True, # 返回字典\n",
    "        output_scores=True, # 返回分数\n",
    "        max_new_tokens=max_len, # 最大长度\n",
    "    )\n",
    "    # 将生成的回复解码（成文字）并打印出\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
    "        if (verbose):\n",
    "            print(output)\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 在微调之前下载模型并进行推理\n",
    "#### 下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"../../chatglm3-6b/\" # 暂时先用智谱的6b的大模型\n",
    "\n",
    "#model_name = \"MediaTek-Research/Breeze-7B-Instruct-v0_1\" \n",
    "# 选择 MediaTek Breeze 7B 模型，因为 TAIDE 模型可能遇到下载次数过多无法访问。\n",
    "# 实际上换其他的开源大模型一样可以，并不影响学习。\n",
    "\n",
    "#model_name = \"/content/taide_7b\" # for colab\n",
    "model_name = \"../../taide_7b\" # 要微调的大模型名称\n",
    "# 等待未来下载成功吧\n",
    "#!wget -O taide_7b.zip \"https://www.dropbox.com/scl/fi/rlzdoidgejt9wiox88gz5/taide_7b.zip?rlkey=vtuc8xq7mq2d0dpuijxlb3jfj&dl=0\"\n",
    "#!unzip taide_7b.zip\n",
    "#os.listdir(\"../../chatglm3-6b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!export HF_ENDPOINT=https://hf-mirror.com\n",
    "#!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 推理\n",
    "查看微调之前模型完成得怎么样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载出现了问题，可能是以下的问题之一: \n",
      "（1）在Mac上加载模型；\n",
      "（2）模型名称错误；\n",
      "（3）其他设置错误.\n"
     ]
    }
   ],
   "source": [
    "'''建议：不修改'''\n",
    "\n",
    "cache_dir = \"cache_file/fine_tuning_cache\"\n",
    "\n",
    "# 模型参数设置 加载模型的方式\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "try:\n",
    "    # 从指定的模型名称或路径载入预训练的语言模型\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name, # 模型名称或路径，这里是路径\n",
    "        cache_dir=cache_dir,\n",
    "        quantization_config=nf4_config,\n",
    "        low_cpu_mem_usage=True,\n",
    "    )\n",
    "except:\n",
    "    print(\"模型加载出现了问题，可能是以下的问题之一: \\n（1）在Mac上加载模型；\\n（2）模型名称错误；\\n（3）其他设置错误.\")\n",
    "# 遇到错误 查了一下大概率是mac上面无法以4bit或者8bit上加载模型\n",
    "# 大概率需要用到GPU 呜呼呼 没有服务器寸步难行哦\n",
    "\n",
    "\n",
    "# 设置名为“transformer”的日志记录器的日志级别为 ERROR。\n",
    "# 这意味着只有严重错误级别及以上的日志消息会被记录下来，\n",
    "# 而低于 ERROR 级别的日志消息（如 DEBUG、INFO 和 WARNING）将被忽略。\n",
    "logging.getLogger(\"transformer\").setLevel(logging.ERROR) \n",
    "# 创建tokenizer并设定结束符号（eos_token）\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    add_eos_token=True,\n",
    "    cache_dir=cache_dir,\n",
    "    quantization_config=nf4_config,\n",
    "    padding_side=\"left\",\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 设定模型推理时需要用到的decoding patameters\n",
    "max_len = 128\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,         # 以采样的方式生成文本 适合诗歌\n",
    "    tempture=0.1,           # 知识问答采用较低的温度\n",
    "    num_beams=1,            # 束搜索宽度 \n",
    "    top_p=0.3,              # 核采样阈值 避免不合理的词汇 并保证文本的随机性\n",
    "    no_repeat_ngram_size=2, # 禁止重复2-gram\n",
    "    pad_token_id=2,         # 填充token对应的id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试不加那些限制参数 竟然成功了\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        pretrained_model_name_or_path=model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不建议更改\n",
    "\n",
    "# 小样示例\n",
    "test_tang_list = ['相見時難別亦難，東風無力百花殘。', '重帷深下莫愁堂，臥後清宵細細長。', '芳辰追逸趣，禁苑信多奇。']\n",
    "\n",
    "# 为每个样本得到模型输出\n",
    "demo_before_fintune = []\n",
    "for tang in test_tang_list:\n",
    "    demo_before_fintune.append(f\"模型输入:\\n以下是一首唐诗的第一句话，请用你的认知判断并完成整首诗。{tang}\\n\\n模型输出:\\n\"\n",
    "                               +evaluate(\"以下是一首唐诗的第一句话，请用你的认知判断并完成整首诗。\",\n",
    "                                         generation_config=generation_config,\n",
    "                                         max_len=max_len,\n",
    "                                         input=tang,\n",
    "                                         verbose=False\n",
    "                                         ))\n",
    "    \n",
    "# 打印并存储结果到文件中\n",
    "for idx in range(len(demo_before_fintune)):\n",
    "    print(f\"Example {idx + 1}\")\n",
    "    print(demo_before_fintune[idx])\n",
    "    print(\"_\" * 80)\n",
    "\n",
    "# 2025/1/8 22:40 推理了30min也没有出现结果 用本地推理还是慢啊\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 设置微调过程的超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''建议修改下面的参数'''\n",
    "num_train_data = 1040 # 设定用于训练的资料数量，可设置的最大值为5000，在大部分情况下希望训练资料越多越好\n",
    "                      # 这会让模型看复哦更多样化的诗句，进而提升生成品质，但也会增加训练的时间\n",
    "                      # 使用预设参数（1040）：fine-tuning大越需要25min，完整跑完所有cell大越需要50min\n",
    "                      # 使用最大值（5000）：fine-tuning大越需要100min，完整跑完所有cell大越需要2h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''必要情况下可以修改下面的代码'''\n",
    "\n",
    "#output_dir = \"/content/drive/MyDrive\" # for colab\n",
    "output_dir = \"hm_save/hm5\"\n",
    "ckpt_dir = \"hm_save/hm5_ckpt\" # 设定model checkpoint存储目录\n",
    "num_epoch = 1 # 设定训练的总Epoch数\n",
    "LEARNING_RATE = 3e-4 # 学习率\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''不建议修改下买的代码'''\n",
    "\n",
    "cache_dir = \"cache_file/fine_tuning_cache\" # 缓存目录\n",
    "from_ckpt = False # 是否从checkpoint载入模型的权重\n",
    "ckpt_name = None # 模型权重文件名\n",
    "dataset_dir = \"hm_dataset/GenAI-Hw5/Tang_training_data.json\"\n",
    "logging_steps = 20 # 20步输出一次训练日志\n",
    "save_steps = 65 # 定义训练过程中每隔多少步骤保存一次模型\n",
    "save_total_limit = 3 # 控制最多保留几个模型checkpoint\n",
    "report_to = None # 设定上报与指标的目标，预设为无\n",
    "MICRO_BATCH_SIZE = 4 # 定义小批次的大小\n",
    "BATCH_SIZE = 16 # 定义一个批次的大小\n",
    "GRADIENT_ACCUMULATION_STEPS = BATCH_SIZE // MICRO_BATCH_SIZE # 计算每个小皮刺累积的梯度布署\n",
    "CUTOFF_LEN = 256 # 设定文本截断的最大长度\n",
    "\n",
    "# LoRA微调参数（此lora非彼lora）\n",
    "LORA_R = 8 # 设定LORA（Layer-wise Random Attention）的R值\n",
    "LORA_ALPHA = 16 # 设定LORA的alpha值     \n",
    "LORA_DROPOUT = 0.05 # 设定LORA的dropout值\n",
    "VAL_SET_SIZE = 0 # 设定验证集大小 预设为无\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\",\n",
    "    \"up_proj\",\n",
    "    \"o_proj\",\n",
    "    \"k_proj\",\n",
    "    \"down_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"v_proj\",\n",
    "]# 设定目标膜组，这些膜组的权重将被保存为checkpoint文件\n",
    "\n",
    "device_map = \"auto\" # 设定设备映射，auto表示自动映射\n",
    "world_size= int(os.environ.get(\"WORLD_SIZE\", 1)) # 获取环境变量WORLD_SIZE的值，如果没有设置，则默认为1\n",
    "ddp = world_size != 1 # 根据world_size的值判断是否使用分布式训练\n",
    "if ddp:\n",
    "    device_map = {\"\": int(os.environ.get(\"LOCAL_RANK\") or 0)}\n",
    "    GRADIENT_ACCUMULATION_STEPS = GRADIENT_ACCUMULATION_STEPS // world_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 开始微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddf0052227e475284b0301e7211bd8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50824a38311447578158be82970bd75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1040 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n# 使用Transformer Trainer进行模型训练\\ntrainer = transformers.Trainer(\\n    model=model, # 要训练的模型\\n    train_dataset=train_data,\\n    eval_dataset=val_data,\\n    args=transformers.TrainingArguments(\\n        per_device_train_batch_size=MICRO_BATCH_SIZE,\\n        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\\n        warmup_steps=50,\\n        num_train_epochs=num_epoch,\\n        learning_rate=LEARNING_RATE,\\n        #fp16=True, # 使用混合精度训练\\n        logging_steps=logging_steps,\\n        save_strategy=\"steps\",\\n        save_steps=save_steps,\\n        output_dir=ckpt_dir,\\n        save_total_limit=save_total_limit,\\n        ddp_find_unused_parameters=False if ddp else None, # 是否使用DDP 控制梯度更新策略\\n        report_to=report_to,\\n    ),\\n    # 处理输入数据，不进行掩码语言建模\\n    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\\n)\\n\\n# 仅用模型的cache功能\\nmodel.config.use_cache = False\\n\\n# 若使用 Pytorch 2.0 以上版本且非 Windows 系统， 进行模型编译\\nif torch.__version__ >= \"2\" and sys.platform != \"win32\":\\n    model = torch.compile(model)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''不建议修改以下代码'''\n",
    "\n",
    "# 创造输出路径\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "# 根据from_ckpt标志，从checkpoint载入模型权重\n",
    "if from_ckpt:\n",
    "    print('Loading from checkpoint:', from_ckpt)\n",
    "    model = PeftModel.from_pretrained(model, ckpt_name)\n",
    "\n",
    "# 将模型准备好以使用INT8训练\n",
    "# 这里估计在mac上也不会行得通\n",
    "config = LoraConfig(\n",
    "    r=LORA_R, # LORA rank           # LoRA的秩 控制低秩矩阵的维度\n",
    "    lora_alpha=LORA_ALPHA,          # 学习率因子 影响LoRA层的学习速率\n",
    "    target_modules=TARGET_MODULES,  # 指定需要应用的LoRA模块列表\n",
    "    lora_dropout=LORA_DROPOUT,      # 设置LoRA层中的dropout概率\n",
    "    bias=\"none\",                    # 不使用偏执项\n",
    "    task_type=\"CAUSAL_LM\"           # 指定任务类型为因果语言模型\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# 将tokenizer的padding token设定为0\n",
    "# pad_token_id 是用于填充序列的特殊标记 ID\n",
    "# 通常在处理不同长度的输入序列时使用，以确保所有序列具有相同的长度\n",
    "tokenizer.pad_token_id = 0\n",
    "\n",
    "# 加载并处理训练数据\n",
    "with open(dataset_dir, \"r\", encoding = \"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "\n",
    "tmp_dataset = \"hm_dataset/GenAI-Hw5/tmp_dataset.json\"\n",
    "with open(tmp_dataset, \"w\", encoding = \"utf-8\") as f:\n",
    "    json.dump(data_json[:num_train_data], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "data = load_dataset(\n",
    "    \"json\", \n",
    "    data_files=tmp_dataset, \n",
    "    download_mode=\"force_redownload\"\n",
    "    )\n",
    "\n",
    "# 将训练数据分为训练集和验证集\n",
    "if VAL_SET_SIZE > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=VAL_SET_SIZE,\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "    )\n",
    "    train_data = train_val[\"train\"].shuffle().map(generate_training_data)\n",
    "    val_data = train_val[\"test\"].shuffle().map(generate_training_data)\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_training_data)\n",
    "    val_data = None\n",
    "#'''\n",
    "# 使用Transformer Trainer进行模型训练\n",
    "trainer = transformers.Trainer(\n",
    "    model=model, # 要训练的模型\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=MICRO_BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        warmup_steps=50,\n",
    "        num_train_epochs=num_epoch,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        #fp16=True, # 使用混合精度训练\n",
    "        logging_steps=logging_steps,\n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=save_steps,\n",
    "        output_dir=ckpt_dir,\n",
    "        save_total_limit=save_total_limit,\n",
    "        ddp_find_unused_parameters=False if ddp else None, # 是否使用DDP 控制梯度更新策略\n",
    "        report_to=report_to,\n",
    "    ),\n",
    "    # 处理输入数据，不进行掩码语言建模\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "# 仅用模型的cache功能\n",
    "model.config.use_cache = False\n",
    "\n",
    "# 若使用 Pytorch 2.0 以上版本且非 Windows 系统， 进行模型编译\n",
    "if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "    model = torch.compile(model)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始训练模型\n",
    "trainer.train()\n",
    "\n",
    "# 将训练完的模型保存到指定的目录中\n",
    "model.save_pretrained(ckpt_dir)\n",
    "\n",
    "# 打印训练过程中可能缺失权重的警告信息\n",
    "print(\"\\n 如果上面有关于丢失关键词的警告，请忽略 :)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试\n",
    "\n",
    "首先加载微调模型ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有可用checkpoint: \n",
      " id: checkpoint name\n"
     ]
    }
   ],
   "source": [
    "'''不建议修改下面代码'''\n",
    "\n",
    "# 找到所有可用checkpoints\n",
    "ckpts = []\n",
    "for ckpt in os.listdir(ckpt_dir):\n",
    "    if (ckpt.startswith(\"checkpoint-\")):\n",
    "        ckpts.append(ckpt)\n",
    "\n",
    "# 罗列所有的checkpoints\n",
    "ckpts = sorted(ckpts, key=lambda ckpt: int(ckpt.split(\"-\")[-1]))\n",
    "print(\"所有可用checkpoint: \")\n",
    "print(\" id: checkpoint name\")\n",
    "for (i, ckpt) in enumerate(ckpts):\n",
    "    print(f\"{i:>3} : {ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''下面的代码可以但是没必要更改'''\n",
    "\n",
    "id_of_ckpt_to_use = -1 # 要用来进行推理的checkpoint的id（对应上一个cell输出的结果）\n",
    "                       # 预设值-1指的是上列checkpoints中倒数第一个 也就是最后一个checkpoint\n",
    "                       # 如果想要选择其他checkpoint 可以把-1改成其他的数字\n",
    "\n",
    "ckpt_name = os.path.join(ckpt_dir, ckpts[id_of_ckpt_to_use])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''下面的代码可以更改但是没必要'''\n",
    "# 可以在这里调整encoding parameter，decoding parameter\n",
    "max_len = 128 # 生成回复的最大长度\n",
    "temperature = 0.1 # 设定生成回复的随机度 值越小生成的回复越稳定\n",
    "top_p = 0.3 # Top-p(nucleus sampling)抽样的概率阈值 用于控制生成回复的多样性\n",
    "#top_k = 3 # 调整Top-k值 以增加生成回复的多样性和避免生成重复的词条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''不推荐下面的代码'''\n",
    "\n",
    "test_data_path = \"hm_dataset/GenAI-Hw5/Tang_testing_data.json\"\n",
    "output_path = os.path.join(output_dir, \"results.txt\")\n",
    "\n",
    "cache_dir = \"cache_dir\\fine_tuing_cache\" # 设定快取目录路径\n",
    "seed = 42 # 设定随机种子 用于重现结果\n",
    "no_repeat_ngram_size = 3 # 设定禁止重复Ngram的大小 用于避免生成重复片段\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 启用四位量化\n",
    "    bnb_4bit_quant_type=\"nf4\", # 指定量化类型为nf4\n",
    "    bnb_4bit_use_double_quant=True, # 启用双重量化\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # 设置计算数据类型为bfloat16\n",
    ")\n",
    "# 这些配置通常用于优化模型在低精度硬件上的性能和内存使用\n",
    "\n",
    "# 使用tokenizer将模型名称转换成模型可识的数组表示形式\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=model_name, # 指定模型名称\n",
    "    cache_dir=cache_dir, # 指定缓存目录\n",
    "    quantization_config=nf4_config, # 应用量化配置\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "# 从预训练模型载入模型并设定为8位整数（INT8）模型\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=nf4_config,\n",
    "    device_map={\"\":0}, # 设定使用的设备 此处指定为GPU0\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "# 从指定的checkpoint早入模型权重\n",
    "model = PeftModel.from_pretrained(model, ckpt_name, device_map={\"\":0})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''不建议修改下面的代码'''\n",
    "\n",
    "results = []\n",
    "\n",
    "# 设定生成配置 包括随机度 束搜索等相关参数\n",
    "generation_config = GenerationConfig(\n",
    "    do_sample=True,\n",
    "    temperature=temperature,\n",
    "    num_beams=1,\n",
    "    top_p=top_p,\n",
    "    #top_k = top_k,\n",
    "    no_repeat_ngram_size=no_repeat_ngram_size,\n",
    "    pad_token_id=2,\n",
    ")\n",
    "\n",
    "# 读取测试资料\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# 对于每个测试资料进行预测 并存下结果\n",
    "#'''\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for (i, test_data) in enumerate(test_data):\n",
    "        predict = evaluate(instruction=test_data[\"instruction\"], \n",
    "                           generation_config=generation_config,\n",
    "                           max_len=max_len,\n",
    "                           input=test_data[\"input\"],\n",
    "                           verbose=False)\n",
    "        f.write(f\"{i+1}. \" + test_data[\"input\"] + predict + \"\\n\")\n",
    "        print(f\"{i+1}. \" + test_data[\"input\"] + predict)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 查看微调模型如何比得上没有微调的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 像之前一样使用相同的测试例子\n",
    "test_tang_list = ['相見時難別亦難，東風無力百花殘。', '重帷深下莫愁堂，臥後清宵細細長。', '芳辰追逸趣，禁苑信多奇。']\n",
    "\n",
    "# 在模型微调之前推理\n",
    "demo_after_finetue = []\n",
    "for tang in test_tang_list:\n",
    "    demo_after_finetue.append(f\"模型输入:\\n以下是一首唐诗的第一句，请用你的知识判断并完成整首诗。{tang}\\\n",
    "                              \\n\\n模型输出:\\n\"+evaluate(instruction=\"以下是一首唐诗的第一句，请用你的知识判断并完成整首诗\",\n",
    "                                                       generation_config=generation_config,\n",
    "                                                       max_len=max_len,\n",
    "                                                       input=tang,\n",
    "                                                       verbose=False))\n",
    "    \n",
    "    # 打印并存储结果到文本文件中\n",
    "    for idx in range(len(demo_after_finetue)):\n",
    "        print(f\"Example {idx + 1}:\")\n",
    "        print(demo_after_finetue[idx])\n",
    "        print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### 分割线\n",
    "下面是我自己的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path = \"hm_dataset/GenAI-Hw5/Tang_testing_data.json\"\n",
    "output_path = os.path.join(output_dir, \"results.txt\")\n",
    "with open(test_data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': '以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。', 'input': '雪霽銀妝素，桔高映瓊枝。'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''在Mac上进行推理的简单函数'''\n",
    "\n",
    "def easy_evaluate(instruction, max_len, input=\"\", verbose=True):\n",
    "    # construct full input prompt\n",
    "    prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. 你是一個樂於助人的助手且擅長寫唐詩。\n",
    "<</SYS>>\n",
    "\n",
    "{instruction}\n",
    "{input}\n",
    "[/INST]\"\"\"\n",
    "    # 將提示文本轉換為模型所需的數字表示形式\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    # 使用模型進行生成回覆\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        #generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_len,\n",
    "    )\n",
    "    # 將生成的回覆解碼並印出\n",
    "    for s in generation_output.sequences:\n",
    "        output = tokenizer.decode(s)\n",
    "        output = output.split(\"[/INST]\")[1].replace(\"</s>\", \"\").replace(\"<s>\", \"\").replace(\"Assistant:\", \"\").replace(\"Assistant\", \"\").strip()\n",
    "        if (verbose):\n",
    "            print(output)\n",
    "\n",
    "    return output\n",
    "\n",
    "output = easy_evaluate(instruction=test_data[0][\"instruction\"],\n",
    "                       max_len=max_len,\n",
    "                       input=test_data[0][\"input\"],\n",
    "                       verbose=True)\n",
    "# 推理速度怎么这么慢哪 二十多分钟都没推理完 难道真的是GPU才可以吗？😭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter name: {name}, Shape: {param.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"../../taide_7b\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 56064\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>>\\nYou are a helpful assistant and good at writing Tang poem. 你是一个乐于助人的助手而且擅长写唐诗。\\n<</SYS>>\\n\\n以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。\\n秦川雄帝宅，函谷壯皇居。\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(dataset_dir, \"r\", encoding = \"utf-8\") as f:\n",
    "    data_json = json.load(f)\n",
    "    \n",
    "prompt = f\"\"\"\\\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant and good at writing Tang poem. 你是一个乐于助人的助手而且擅长写唐诗。\n",
    "<</SYS>>\n",
    "\n",
    "{data_json[0][\"instruction\"]}\n",
    "{data_json[0][\"input\"]}\n",
    "\"\"\"\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s = tokenizer(\n",
    "    prompt,\n",
    "    truncation=True,\n",
    "    max_length=CUTOFF_LEN + 1,\n",
    "    padding=\"max_length\",\n",
    ")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s> [INST] <<SYS>>\\nYou are a helpful assistant and good at writing Tang poem. 你是一个乐于助人的助手而且擅长写唐诗。\\n<</SYS>>\\n\\n以下是一首唐詩的第一句話，請用你的知識判斷並完成整首詩。\\n秦川雄帝宅，函谷壯皇居。\\n</s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
