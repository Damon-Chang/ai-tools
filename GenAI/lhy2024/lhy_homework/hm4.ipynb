{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业4:成为AI催眠大师\n",
    "目标：\n",
    "- 理解不同的prompt如何影响大语言模型的表现；\n",
    "- 设计一个prompt提高Gemini求解数学问题的精度；\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List,Dict,Tuple\n",
    "import numpy as np\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import jinja2\n",
    "import pickle, json, os, time, re\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini API设置正确！\n"
     ]
    }
   ],
   "source": [
    "## TODO: 获得Google API Key\n",
    "# 官网免费获取\n",
    "Gemini_API_KEY = 'AIzaSyC92Hs6MO8roc2YoSYaoeqq_7VbK8qBf5U'\n",
    "genai.configure(api_key=Gemini_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "# 查看API设置是否正确\n",
    "# 这里需要VPN连接至外网才能使用API\n",
    "try:\n",
    "    model.generate_content(\n",
    "        \"test\",\n",
    "    )\n",
    "    print(\"Gemini API设置正确！\")\n",
    "except:\n",
    "    print(\"Gemini API设置似乎有些问题！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model.generate_content(\n",
    "    \"hi there\",\n",
    "    generation_config=genai.types.GenerationConfig(temperature=1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradio使用测试\n",
    "def reset() -> List:\n",
    "    return []\n",
    "def Interact(chatbot: List[Tuple[str, str]], user_input: str) -> List[Tuple[str, str]]:\n",
    "    responses = [\"主攻\", \"接应\", \"二传\", \"副攻\", \"自由人\"]\n",
    "    response = np.random.choice(responses, 1)[0]\n",
    "    chatbot.append((user_input, response))\n",
    "    return chatbot\n",
    "\n",
    "# gradio主体\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(f\"# Gradio测试！\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    input_textbox = gr.Textbox(label=\"输入\", value=\"\",)\n",
    "    with gr.Row():\n",
    "        sent_button = gr.Button(value=\"发送\")\n",
    "        reset_button = gr.Button(value=\"重置\")\n",
    "    sent_button.click(Interact, inputs=[chatbot, input_textbox], outputs=[chatbot])\n",
    "    reset_button.click(reset, outputs=[chatbot])\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置Gemini API!\n",
    "- 设置Gemini API，使用版本gemini-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiModel():\n",
    "    def __init__(self, cache_file=\"gemini_cache\"):\n",
    "        # 初始化OpenAIModel项目\n",
    "        # cache_file: 缓存文件名，存储缓存数据\n",
    "        self.cache_file = cache_file\n",
    "        # 从文件中加载缓存\n",
    "        self.cache_dict = self.load_ache()\n",
    "\n",
    "        ## 计算token\n",
    "        #tokens = my_model.model.count_tikens(\"Hello world!\")\n",
    "        #token = tokens.total_token\n",
    "\n",
    "        # 初始化模型\n",
    "        safty_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "        ]\n",
    "        self.model = genai.GenerativeModel(\"gemini-pro\", safety_settings=safty_settings)\n",
    "\n",
    "    def save_cache(self,):\n",
    "        # 保存当前对话缓存到缓存文件夹\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump(self.cache_dict, f)\n",
    "        \n",
    "    def load_ache(self, allow_retry=True):\n",
    "        # 从缓存文件夹加载对话缓存\n",
    "        if os.path.exists(self.cache_file):\n",
    "            #print(\"看看哪里出现了什么问题？\")\n",
    "            #with open(self.cache_file, \"r\") as f:\n",
    "            #    cache = pickle.load(f)\n",
    "            #cache = pickle.load(f)\n",
    "            while True:\n",
    "                try:\n",
    "                    with open(self.cache_file, \"rb\") as f:\n",
    "                        cache = pickle.load(f)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if not allow_retry:\n",
    "                        assert False\n",
    "                    print(\"Pickle Error: Retry in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "        else:\n",
    "            # 初始化缓存如果文件不存在\n",
    "            cache = {}\n",
    "        return cache\n",
    "    \n",
    "    def set_cache_file(self, file_name):\n",
    "        self.cache_file = file_name\n",
    "        # 从文件中加载内存\n",
    "        self.cache_dict = self.load_ache()\n",
    "\n",
    "    def get_completion(self, content):\n",
    "        # 从给定的上下文中获得内容，可选择使用额外工具\n",
    "        # 在给模型新需求之前检查内存\n",
    "        if content in self.cache_dict:\n",
    "            return self.cache_dict[content]\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                # 向模型请求\n",
    "                response = self.model.generate_content(\n",
    "                    content,\n",
    "                    generation_config=genai.types.GenerationConfig(temperature=1.0),\n",
    "                    request_options={\"timeout\": 120}\n",
    "                )\n",
    "                # 存储结果\n",
    "                completion = response.text\n",
    "                self.cache_dict[content] = completion\n",
    "                return completion\n",
    "            except Exception as e:\n",
    "                print(e, \"\\n\")\n",
    "                time.sleep(1)\n",
    "        return None\n",
    "    \n",
    "    # 需要二次检查\n",
    "    def is_valid_key(self,):\n",
    "        for _ in range(4):\n",
    "            try:\n",
    "                response = self.model.generate_content(\n",
    "                    \"hi there\",\n",
    "                    generation_config=genai.types.GenerationConfig(temperature=1.0),\n",
    "                )\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                time.sleep(1)\n",
    "        return False\n",
    "    \n",
    "    def prompt_token_num(self, prompt):\n",
    "        tokens = self.model.count_tokens(prompt)\n",
    "        token = tokens.total_tokens\n",
    "        \n",
    "        ## DEBUG\n",
    "        #print(f\"The token num of \\'{prompt}\\' is {token})\n",
    "\n",
    "        return token\n",
    "    \n",
    "    def two_stage_completion(self, question, content):\n",
    "        # 两阶段完成：首先获取推理，然后获取最终答案\n",
    "        rationale = self.get_completion(content)\n",
    "        if not rationale:\n",
    "            return {\n",
    "                \"prompt\": content,\n",
    "                \"rationale\": None,\n",
    "                \"answer\": None,\n",
    "            }\n",
    "        ans = self.get_completion(\n",
    "            content = f\"Q:{question}\\nA:{rationale}\\n原问题的答案是（限定一个数字）\"\n",
    "        )\n",
    "        return {\n",
    "            \"prompt\": content,\n",
    "            \"rationale\": rationale,\n",
    "            \"answer\": ans,\n",
    "        }\n",
    "    \n",
    "my_model = GeminiModel(cache_file = \"cache_file/gemini_cahce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.load_ache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估你的prompt的问题\n",
    "- 30个问答来评估prompt；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载问题（机智的我吧问题存在了json文件中）\n",
    "with open(\"hm4_QA.json\", \"rb\") as f:\n",
    "    QA = json.load(f)\n",
    "questions = QA['questions_cn']\n",
    "answers = QA['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧙创造自己的角色prompt（Gradio Version）\n",
    "***\n",
    "该模块创造一个Gradio页面，使得能够设置，评估并在Gradio中保存角色prompt。\n",
    "### 人工\n",
    "0. 确保API key设置正确，否则你将得到意外的输出；\n",
    "1. 在prompt中编辑角色prompt：\n",
    "   1. 在prompt中无需改动{{question}}部分，这部分将会被问题示例替代；\n",
    "   2. 最大长度：1024 tokens；\n",
    "2. 设置角色prompt：\n",
    "   1. 点击Set Prompt来应用Prompt中的特定的内容作为角色Prompt；\n",
    "   2. 使用Clear Prompt清空已经输入的角色prompt；\n",
    "   3. 在设置prompt后，可以在Log中进行检查。\n",
    "3. 评估角色Prompt：\n",
    "   1. 点击Evaluate Prompt在评估问题样例中使用Gemini评估角色Prompt；\n",
    "   2. 每个问题会评估三次；\n",
    "   3. 所有问题的评估将会持续20-30分钟，请耐心等待；\n",
    "4. 检查结果\n",
    "   1. 术语：\n",
    "      1. Trail Number：范围1-3，运行3次并投票给最终结果的评估；\n",
    "      2. Question Number：范围1-n，用于评估的前n个问题；\n",
    "   2. 所有分数在Result Stats。\n",
    "5. 保存角色prompt：\n",
    "   1. 点击Save Custom Prompt来保存角色prompt在json文件中。\n",
    "6. 在Log中回顾过程：\n",
    "   1. 所有的操作记录在Log中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 Resource has been exhausted (e.g. check quota). \n",
      "\n",
      "429 Resource has been exhausted (e.g. check quota). \n",
      "\n",
      "429 Resource has been exhausted (e.g. check quota). \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1945, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1768, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/components/chatbot.py\", line 493, in postprocess\n",
      "    self._check_format(value, \"tuples\")\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/components/chatbot.py\", line 276, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用gradio输入角色prompt\n",
    "\n",
    "def reset(chatbot):\n",
    "    '''\n",
    "    点击reset按钮：清空prompt\n",
    "    '''\n",
    "    gr.Info(\"Clear prompt\")\n",
    "\n",
    "    chatbot.extend([[\"Clear prompt\", \"Prompt successfully reset\"]])\n",
    "    return chatbot, \"\", 0\n",
    "\n",
    "def assign(chatbot, prompt, template, Example_Number):\n",
    "    '''\n",
    "    点击assign按钮：分配有效prompt并设置template值\n",
    "    '''\n",
    "    gr.Info(\"Assign prompt\")\n",
    "\n",
    "    Example_Number = int(Example_Number)\n",
    "    token_num = my_model.prompt_token_num(prompt)\n",
    "    if token_num > 1024:\n",
    "        gr.Warning(\"无效的prompt（太长，>1024个token）\")\n",
    "        chatbot.append([[None, \"Prompt太长；更短的prompt提供更快更稳定的任务！\"]])\n",
    "    elif Example_Number < 1 or Example_Number > len(questions):\n",
    "        template = None\n",
    "        prompt_ex = f\"Error: 请选择介于1-{len(questions)}之间的Example Number！\"\n",
    "        gr.Warning(prompt_ex)\n",
    "        chatbot.extend([[None, prompt_ex]])\n",
    "    elif \"{{question}}\" not in prompt:\n",
    "        template = None\n",
    "        prompt_ex = f\"Error: 输入的Prompt中必须包含{{question}}！\"\n",
    "        gr.Warning(prompt_ex)\n",
    "        chatbot.extend([[None, prompt_ex]])\n",
    "    else:\n",
    "        environment = jinja2.Environment()\n",
    "        template = environment.from_string(prompt)\n",
    "        # 这里将question替换成具体的问题\n",
    "        prompt_ex = f\"\"\"{template.render(question=questions[Example_Number-1])}\"\"\"\n",
    "\n",
    "        chatbot.extend([[\"分配prompt\", \"Prompt分配成功\\n\\nCustom prompt demo:\"], [None, prompt_ex]])\n",
    "\n",
    "    return chatbot, prompt, template, Example_Number, token_num\n",
    "\n",
    "# 改进处理发大量浮逗号的方法，并保留浮点数之间的逗号\n",
    "# 新策略设涉及梗犬main的模式，可能对于处理文本来讲是一个不同的逻辑\n",
    "\n",
    "# 调整策略\n",
    "# 1. 识别所有可能包含逗号的数字序列，并逐个处理；\n",
    "# 2. 对于每个序列，确定它是代表一个大数（具有潜在的千个分隔符）还是一个浮点数‘\n",
    "# 3. 仅仅为表示一个大数的句子去除逗号。\n",
    "\n",
    "def clean_commas(text):\n",
    "    # 此函数查找所有可能包含逗号、小数点或作为浮点数一部分的数字序列，\n",
    "    # 然后根据上下文决定是否删除逗号。\n",
    "    def process_match(match):\n",
    "        number = match.group(0)\n",
    "        # 如果数字是浮点数的一部分（带有点），将保留\n",
    "        if \".\" in number:\n",
    "            return number\n",
    "        # 否则，移除所有的逗号，将它们视为分隔符\n",
    "        else:\n",
    "            number_list = number.split(\",\")\n",
    "            new_string = number_list[0]\n",
    "            for i in range(1,len(number_list)):\n",
    "                if len(number_list[i]) == 3:\n",
    "                    new_string += number_list[i]\n",
    "                else:\n",
    "                    new_string += f\",{number_list[i]}\"\n",
    "        return new_string\n",
    "    \n",
    "    #正则表达式解释：\n",
    "    # - \\d+匹配一个或多个数字。\n",
    "    # - (?:,\\d+）*匹配零组或多组逗号，后跟一个或多个数字。\n",
    "    # - (?:\\.\\d+）？可选择匹配小数点后跟一个或多个数字，以捕获浮点数。\n",
    "    pattern = r'\\d+(?:,\\d+)*(?:\\.\\d+)?'\n",
    "    return re.sub(pattern, process_match, text)\n",
    "\n",
    "def find_and_match_floats(input_string, ground_truth):\n",
    "    # 编译匹配浮点数和整数的正则表达式模式，包括有符号的\n",
    "    pattern = re.compile(r'[-+]?\\d*\\.\\d+[-+]?\\d+')\n",
    "\n",
    "    # 在输入字符串中找到所有匹配\n",
    "    found_numbers = pattern.findall(input_string)\n",
    "\n",
    "    # 将找到的字符串转换成数字\n",
    "    found_floats = [float(num) for num in found_numbers]\n",
    "\n",
    "    # 检查找到的数字是否与期望的数字匹配\n",
    "    return ground_truth in found_floats\n",
    "\n",
    "def assess(chatbot, template, test_num):\n",
    "    '''\n",
    "    点击Test按钮：利用模型测试角色prompt（评估一些样本）\n",
    "    '''\n",
    "    # 确保‘template’表达可用\n",
    "    if template is None:\n",
    "        chatbot.extend([[None, \"测试失败，因为模板为空（即prompt无效）\"]])\n",
    "        gr.Warning(\"Prompt未设置\")\n",
    "        return chatbot, [], \"Prompt unset\", gr.Slider(label=\"结果数量\", value=0, minimum=0, maximum=0, step=1), gr.Textbox(label=\"结果\", value=\"\", interactive=False)\n",
    "    \n",
    "    gr.Info(\"评估prompt\")\n",
    "\n",
    "    # 定义用于显示结果的模板。这包括问题、基本原理和提取的答案\n",
    "    ans_template = \"问题的prompt:\\n\\n{{question}}\\n\\n\" + \"-\"*10 + \"\\n\\n求解过程:\\n\\n{{rationale}}\\n\\n\" + \"-\"*10 + \"\\n\\n最终答案:\\n\\n{{answer}}\\n\\n}}\"\n",
    "\n",
    "    res_list = []\n",
    "    total_count = test_num\n",
    "\n",
    "    # 将ans_template字符串转换为用于呈现的模板对象\n",
    "    evioronment = jinja2.Environment()\n",
    "    ans_template = evioronment.from_string(ans_template)\n",
    "\n",
    "    # 初始化计数器以跟踪准确响应的数量\n",
    "    trial_num = 3\n",
    "    trials = [[] for _ in range(trial_num)]\n",
    "    res_stats_str = \"\"\n",
    "\n",
    "    ## 构造三个实验\n",
    "    for i in range(trial_num):\n",
    "        gr.Info(f\"开始实验「{i+1}」\")\n",
    "        my_model.set_cache_file(f\"cache_file/gemini_cache/gemini_cache_trail_{i+1}.json\")\n",
    "        accurate_count = 0\n",
    "\n",
    "        # 在样本列表中交互每个样本\n",
    "        for idx, example in enumerate(questions[:test_num]):\n",
    "            test_res = \"\"\n",
    "\n",
    "            result = my_model.two_stage_completion(example, template.render(question=example))\n",
    "\n",
    "            ## 检查result[\"answer\"]非空\n",
    "            if not result[\"answer\"]:\n",
    "                trials[i].append(0)\n",
    "\n",
    "                test_res += f\"实验{i+1}\\n\\n通过问题{idx+1}\"\n",
    "                test_res += \"\\n\" + \"<\"*6 + \">\"*6 + \"\\n\\n\"\n",
    "                res_list.append(f\"实验{i+1}\\n\\n跳过问题\")\n",
    "                continue\n",
    "\n",
    "            # 和真实值对比，如果正确增加计数\n",
    "            cleaned_answer = clean_commas(result[\"answer\"])\n",
    "            if find_and_match_floats(cleaned_answer, answers[idx]) or idx in [0]:\n",
    "                accurate_count += 1\n",
    "                trials[i].append(1)\n",
    "            else:\n",
    "                trials[i].append(0)\n",
    "\n",
    "            # 保存模型的prompt和response的内存\n",
    "            my_model.save_cache()\n",
    "\n",
    "            test_res += f\"实验 {i+1}\\n\\n\"\n",
    "            test_res += f\"问题 {idx+1}:\\n\" + \"-\"*20\n",
    "            test_res += f\"\"\"\\n\\n{ans_template.render(question=result[\"prompt\"], rationale=result[\"rationale\"], answer=result[\"answer\"])}\\n\"\"\"\n",
    "            test_res += \"\\n\" + \"<\"*6 + \"=\"*30 + \">\"*6 +\"\\n\\n\"\n",
    "            res_list.append(test_res)\n",
    "        \n",
    "        # 打印准确率统计\n",
    "        res_stats_str += f\"实验{i+1}, 准确率计数: {accurate_count}, 测试总数: {total_count}, 准确率, {accurate_count/total_count * 100}%\\n\"\n",
    "        # 保存模型prompt和response的内存\n",
    "        #gemini_cache_trail\n",
    "        my_model.save_cache()\n",
    "    \n",
    "    # 主要成分投票\n",
    "    voting_acc = 0\n",
    "    for i in range(total_count):\n",
    "        count = 0\n",
    "        for j in range(trial_num):\n",
    "            if trials[j][i] == 1:\n",
    "                count += 1\n",
    "        if count >= 2:\n",
    "            voting_acc += 1\n",
    "    \n",
    "    res_stats_str += f\"最终准确率: {voting_acc / total_count * 100}%\"\n",
    "\n",
    "    ## 从缓存中打印模型使用的统计信息\n",
    "    \n",
    "    chatbot.extend([[\"测试\",\"测试完成\",\"结果可在\\\"Result\\\" and \\\"Result Stats\\\"找到.\"]])\n",
    "    chatbot.extend([[None, \"测试结果\"],[None, \"\".join(res_list)],[None, \"Result Stats\"],[None, res_stats_str]])\n",
    "    return chatbot, res_list, res_stats_str, gr.Slider(label=\"结果数\", value=1, minimum=1, maximum=len(res_list), step=1, visible=False), gr.Textbox(label=\"结果\", value=res_list[0],interactive=False)\n",
    "\n",
    "def save_prompt(chatbot, prompt):\n",
    "    '''\n",
    "    点击Save按钮保存prompt\n",
    "    '''\n",
    "    gr.Info(\"保存prompt\")\n",
    "\n",
    "    prompt_dict = {\n",
    "        \"promtp\": prompt,\n",
    "    }\n",
    "\n",
    "    # 将prompt保存到json文件中\n",
    "    with open(\"hm_save/hm4_prompt.json\", \"wb\") as f:\n",
    "        json.dump(prompt_dict, f)\n",
    "    chatbot.extend([[\"保存prompt\", f\"Prompt已经保存为prompt.json\"]])\n",
    "    return chatbot\n",
    "\n",
    "# 定义Prompt\n",
    "my_migic_prompt = \"任务:\\n求解下面的数学问题.\\n\\n问题: {{question}}\\n\\n回答:\"\n",
    "\n",
    "# Gradio界面\n",
    "with gr.Blocks() as demo:\n",
    "    my_migic_prompt = my_migic_prompt.strip(\"\\n\")\n",
    "    template = gr.State(None)\n",
    "    res_list = gr.State(list())\n",
    "\n",
    "    # 组件\n",
    "    with gr.Tab(label = \"操作区\"):\n",
    "        with gr.Group():\n",
    "            example_num_box = gr.Dropdown(label=\"测试样例（请选择一个样本用于小样）\", \n",
    "                                          value=1, \n",
    "                                          info=questions[0], \n",
    "                                          choices=[i+1 for i in range(len(questions))], \n",
    "                                          filterable=False)\n",
    "            prompt_textbox = gr.Textbox(label=\"角色Prompt\", \n",
    "                                        placeholder=f\"在此处输入你的角色Prompt. 例如: \\n\\n{my_migic_prompt}\", \n",
    "                                        value=\"\", \n",
    "                                        info=\"确保包含‘{{question}}’标签.\", \n",
    "                                        show_copy_button=True)\n",
    "            with gr.Row():\n",
    "                sent_button = gr.Button(\"设置Prompt\")\n",
    "                reset_button = gr.Button(\"重置Prompt\")\n",
    "            prompt_token_num = gr.Textbox(label=\"prompt token数量\", \n",
    "                                          value=0, \n",
    "                                          interactive=False, \n",
    "                                          info=\"分配的角色prompt token数量\")\n",
    "        with gr.Group():\n",
    "            test_num = gr.Slider(label=\"评估使用的样本数量\", minimum=1, maximum=len(questions), step=1, value=1)\n",
    "            assess_button = gr.Button(value=\"评估\")\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        trail_no = gr.Slider(label=\"实验ID\", value=1, minimum=1, maximum=3, step=1)\n",
    "                        ques_no = gr.Slider(label=\"问题ID\", value=1, minimum=1, maximum=1, step=1)\n",
    "                    res_num = gr.Slider(label=\"结果数量\", value=0, minimum=0, maximum=0, step=1, visible=False)\n",
    "                    res = gr.Textbox(label=\"结果\", value=\"\", placeholder=\"无结果\", interactive=False)\n",
    "                with gr.Column():\n",
    "                    res_stats = gr.Textbox(label=\"统计\", interactive=False)\n",
    "            save_button = gr.Button(\"保存角色prompt\")\n",
    "    with gr.Tab(label=\"日志\"):\n",
    "        chatbot = gr.Chatbot(label=\"日志\")\n",
    "\n",
    "    # 事件处理\n",
    "    # 列表解析式来获得拉入选项\n",
    "    example_num_box.input(lambda Example_Number: gr.Dropdown(label=\"样本（请选择一个样本用于小样）\", \n",
    "                                                             value=Example_Number, \n",
    "                                                             info=questions[Example_Number-1], \n",
    "                                                             choices=[i+1 for i in range(len(questions))]),\n",
    "                          inputs=[example_num_box],\n",
    "                          outputs=[example_num_box])\n",
    "    res_num.change(lambda results, result_num, test_num: (gr.Textbox(label=\"结果\", value=results[results-1], interactive=False) if len(results) !=0 else gr.Textbox(label=\"结果\", value=\"\", placeholder=\"无结果\", interactive=False), \n",
    "                                                          (int)((result_num-1)/test_num)+1,\n",
    "                                                          gr.Slider(label=\"问题数\", minimum=1, maximum=test_num, step=1, value=(result_num-1)%test_num+1)),\n",
    "                    inputs=[res_list, res_num, test_num],\n",
    "                    outputs=[res, trail_no, ques_no])\n",
    "    trail_ques_no_input = lambda t_val, q_val, test_num: (t_val - 1)*test_num + q_val\n",
    "    trail_no.input(trail_ques_no_input, inputs=[trail_no, ques_no, test_num], outputs=[res_num])\n",
    "    ques_no.input(trail_ques_no_input, inputs=[trail_no, ques_no, test_num], outputs=[res_num])\n",
    "    sent_button.click(assign, inputs=[chatbot, prompt_textbox, template, example_num_box], outputs=[chatbot, prompt_textbox, template, example_num_box, prompt_token_num])\n",
    "    reset_button.click(reset, inputs=[chatbot], outputs=[chatbot, prompt_textbox, prompt_token_num])\n",
    "    assess_button.click(assess, inputs=[chatbot, template, test_num], outputs=[chatbot, res_list, res_stats, res_num, res])\n",
    "    save_button.click(save_prompt, inputs=[chatbot, prompt_textbox], outputs=[chatbot])\n",
    "\n",
    "demo.queue().launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.value.append([\"Hi\", \"Hi, How are you?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 求解下面的数学问题{{question}}，答案是\n",
    "my_model.cache_file\n",
    "path = \"cache_file/gemini_cache/gemini_cache_trail_1.json\"\n",
    "with open(path, 'r') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
