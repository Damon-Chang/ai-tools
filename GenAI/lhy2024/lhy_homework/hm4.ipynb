{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½œä¸š4:æˆä¸ºAIå‚¬çœ å¤§å¸ˆ\n",
    "ç›®æ ‡ï¼š\n",
    "- ç†è§£ä¸åŒçš„promptå¦‚ä½•å½±å“å¤§è¯­è¨€æ¨¡å‹çš„è¡¨ç°ï¼›\n",
    "- è®¾è®¡ä¸€ä¸ªpromptæé«˜Geminiæ±‚è§£æ•°å­¦é—®é¢˜çš„ç²¾åº¦ï¼›\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from typing import List,Dict,Tuple\n",
    "import numpy as np\n",
    "import openai\n",
    "import google.generativeai as genai\n",
    "import jinja2\n",
    "import pickle, json, os, time, re\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini APIè®¾ç½®æ­£ç¡®ï¼\n"
     ]
    }
   ],
   "source": [
    "## TODO: è·å¾—Google API Key\n",
    "# å®˜ç½‘å…è´¹è·å–\n",
    "Gemini_API_KEY = 'AIzaSyC92Hs6MO8roc2YoSYaoeqq_7VbK8qBf5U'\n",
    "genai.configure(api_key=Gemini_API_KEY)\n",
    "model = genai.GenerativeModel(\"gemini-pro\")\n",
    "# æŸ¥çœ‹APIè®¾ç½®æ˜¯å¦æ­£ç¡®\n",
    "# è¿™é‡Œéœ€è¦VPNè¿æ¥è‡³å¤–ç½‘æ‰èƒ½ä½¿ç”¨API\n",
    "try:\n",
    "    model.generate_content(\n",
    "        \"test\",\n",
    "    )\n",
    "    print(\"Gemini APIè®¾ç½®æ­£ç¡®ï¼\")\n",
    "except:\n",
    "    print(\"Gemini APIè®¾ç½®ä¼¼ä¹æœ‰äº›é—®é¢˜ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = model.generate_content(\n",
    "    \"hi there\",\n",
    "    generation_config=genai.types.GenerationConfig(temperature=1.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradioä½¿ç”¨æµ‹è¯•\n",
    "def reset() -> List:\n",
    "    return []\n",
    "def Interact(chatbot: List[Tuple[str, str]], user_input: str) -> List[Tuple[str, str]]:\n",
    "    responses = [\"ä¸»æ”»\", \"æ¥åº”\", \"äºŒä¼ \", \"å‰¯æ”»\", \"è‡ªç”±äºº\"]\n",
    "    response = np.random.choice(responses, 1)[0]\n",
    "    chatbot.append((user_input, response))\n",
    "    return chatbot\n",
    "\n",
    "# gradioä¸»ä½“\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(f\"# Gradioæµ‹è¯•ï¼\")\n",
    "    chatbot = gr.Chatbot()\n",
    "    input_textbox = gr.Textbox(label=\"è¾“å…¥\", value=\"\",)\n",
    "    with gr.Row():\n",
    "        sent_button = gr.Button(value=\"å‘é€\")\n",
    "        reset_button = gr.Button(value=\"é‡ç½®\")\n",
    "    sent_button.click(Interact, inputs=[chatbot, input_textbox], outputs=[chatbot])\n",
    "    reset_button.click(reset, outputs=[chatbot])\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®¾ç½®Gemini API!\n",
    "- è®¾ç½®Gemini APIï¼Œä½¿ç”¨ç‰ˆæœ¬gemini-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeminiModel():\n",
    "    def __init__(self, cache_file=\"gemini_cache\"):\n",
    "        # åˆå§‹åŒ–OpenAIModelé¡¹ç›®\n",
    "        # cache_file: ç¼“å­˜æ–‡ä»¶åï¼Œå­˜å‚¨ç¼“å­˜æ•°æ®\n",
    "        self.cache_file = cache_file\n",
    "        # ä»æ–‡ä»¶ä¸­åŠ è½½ç¼“å­˜\n",
    "        self.cache_dict = self.load_ache()\n",
    "\n",
    "        ## è®¡ç®—token\n",
    "        #tokens = my_model.model.count_tikens(\"Hello world!\")\n",
    "        #token = tokens.total_token\n",
    "\n",
    "        # åˆå§‹åŒ–æ¨¡å‹\n",
    "        safty_settings = [\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "            {\n",
    "                \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                \"threshold\": \"BLOCK_NONE\",\n",
    "            },\n",
    "        ]\n",
    "        self.model = genai.GenerativeModel(\"gemini-pro\", safety_settings=safty_settings)\n",
    "\n",
    "    def save_cache(self,):\n",
    "        # ä¿å­˜å½“å‰å¯¹è¯ç¼“å­˜åˆ°ç¼“å­˜æ–‡ä»¶å¤¹\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            pickle.dump(self.cache_dict, f)\n",
    "        \n",
    "    def load_ache(self, allow_retry=True):\n",
    "        # ä»ç¼“å­˜æ–‡ä»¶å¤¹åŠ è½½å¯¹è¯ç¼“å­˜\n",
    "        if os.path.exists(self.cache_file):\n",
    "            #print(\"çœ‹çœ‹å“ªé‡Œå‡ºç°äº†ä»€ä¹ˆé—®é¢˜ï¼Ÿ\")\n",
    "            #with open(self.cache_file, \"r\") as f:\n",
    "            #    cache = pickle.load(f)\n",
    "            #cache = pickle.load(f)\n",
    "            while True:\n",
    "                try:\n",
    "                    with open(self.cache_file, \"rb\") as f:\n",
    "                        cache = pickle.load(f)\n",
    "                    break\n",
    "                except Exception:\n",
    "                    if not allow_retry:\n",
    "                        assert False\n",
    "                    print(\"Pickle Error: Retry in 5 seconds...\")\n",
    "                    time.sleep(5)\n",
    "        else:\n",
    "            # åˆå§‹åŒ–ç¼“å­˜å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨\n",
    "            cache = {}\n",
    "        return cache\n",
    "    \n",
    "    def set_cache_file(self, file_name):\n",
    "        self.cache_file = file_name\n",
    "        # ä»æ–‡ä»¶ä¸­åŠ è½½å†…å­˜\n",
    "        self.cache_dict = self.load_ache()\n",
    "\n",
    "    def get_completion(self, content):\n",
    "        # ä»ç»™å®šçš„ä¸Šä¸‹æ–‡ä¸­è·å¾—å†…å®¹ï¼Œå¯é€‰æ‹©ä½¿ç”¨é¢å¤–å·¥å…·\n",
    "        # åœ¨ç»™æ¨¡å‹æ–°éœ€æ±‚ä¹‹å‰æ£€æŸ¥å†…å­˜\n",
    "        if content in self.cache_dict:\n",
    "            return self.cache_dict[content]\n",
    "        for _ in range(3):\n",
    "            try:\n",
    "                # å‘æ¨¡å‹è¯·æ±‚\n",
    "                response = self.model.generate_content(\n",
    "                    content,\n",
    "                    generation_config=genai.types.GenerationConfig(temperature=1.0),\n",
    "                    request_options={\"timeout\": 120}\n",
    "                )\n",
    "                # å­˜å‚¨ç»“æœ\n",
    "                completion = response.text\n",
    "                self.cache_dict[content] = completion\n",
    "                return completion\n",
    "            except Exception as e:\n",
    "                print(e, \"\\n\")\n",
    "                time.sleep(1)\n",
    "        return None\n",
    "    \n",
    "    # éœ€è¦äºŒæ¬¡æ£€æŸ¥\n",
    "    def is_valid_key(self,):\n",
    "        for _ in range(4):\n",
    "            try:\n",
    "                response = self.model.generate_content(\n",
    "                    \"hi there\",\n",
    "                    generation_config=genai.types.GenerationConfig(temperature=1.0),\n",
    "                )\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                traceback.print_exc()\n",
    "                time.sleep(1)\n",
    "        return False\n",
    "    \n",
    "    def prompt_token_num(self, prompt):\n",
    "        tokens = self.model.count_tokens(prompt)\n",
    "        token = tokens.total_tokens\n",
    "        \n",
    "        ## DEBUG\n",
    "        #print(f\"The token num of \\'{prompt}\\' is {token})\n",
    "\n",
    "        return token\n",
    "    \n",
    "    def two_stage_completion(self, question, content):\n",
    "        # ä¸¤é˜¶æ®µå®Œæˆï¼šé¦–å…ˆè·å–æ¨ç†ï¼Œç„¶åè·å–æœ€ç»ˆç­”æ¡ˆ\n",
    "        rationale = self.get_completion(content)\n",
    "        if not rationale:\n",
    "            return {\n",
    "                \"prompt\": content,\n",
    "                \"rationale\": None,\n",
    "                \"answer\": None,\n",
    "            }\n",
    "        ans = self.get_completion(\n",
    "            content = f\"Q:{question}\\nA:{rationale}\\nåŸé—®é¢˜çš„ç­”æ¡ˆæ˜¯ï¼ˆé™å®šä¸€ä¸ªæ•°å­—ï¼‰\"\n",
    "        )\n",
    "        return {\n",
    "            \"prompt\": content,\n",
    "            \"rationale\": rationale,\n",
    "            \"answer\": ans,\n",
    "        }\n",
    "    \n",
    "my_model = GeminiModel(cache_file = \"cache_file/gemini_cahce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_model.load_ache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¯„ä¼°ä½ çš„promptçš„é—®é¢˜\n",
    "- 30ä¸ªé—®ç­”æ¥è¯„ä¼°promptï¼›"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŠ è½½é—®é¢˜ï¼ˆæœºæ™ºçš„æˆ‘å§é—®é¢˜å­˜åœ¨äº†jsonæ–‡ä»¶ä¸­ï¼‰\n",
    "with open(\"hm4_QA.json\", \"rb\") as f:\n",
    "    QA = json.load(f)\n",
    "questions = QA['questions_cn']\n",
    "answers = QA['answers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§™åˆ›é€ è‡ªå·±çš„è§’è‰²promptï¼ˆGradio Versionï¼‰\n",
    "***\n",
    "è¯¥æ¨¡å—åˆ›é€ ä¸€ä¸ªGradioé¡µé¢ï¼Œä½¿å¾—èƒ½å¤Ÿè®¾ç½®ï¼Œè¯„ä¼°å¹¶åœ¨Gradioä¸­ä¿å­˜è§’è‰²promptã€‚\n",
    "### äººå·¥\n",
    "0. ç¡®ä¿API keyè®¾ç½®æ­£ç¡®ï¼Œå¦åˆ™ä½ å°†å¾—åˆ°æ„å¤–çš„è¾“å‡ºï¼›\n",
    "1. åœ¨promptä¸­ç¼–è¾‘è§’è‰²promptï¼š\n",
    "   1. åœ¨promptä¸­æ— éœ€æ”¹åŠ¨{{question}}éƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†å°†ä¼šè¢«é—®é¢˜ç¤ºä¾‹æ›¿ä»£ï¼›\n",
    "   2. æœ€å¤§é•¿åº¦ï¼š1024 tokensï¼›\n",
    "2. è®¾ç½®è§’è‰²promptï¼š\n",
    "   1. ç‚¹å‡»Set Promptæ¥åº”ç”¨Promptä¸­çš„ç‰¹å®šçš„å†…å®¹ä½œä¸ºè§’è‰²Promptï¼›\n",
    "   2. ä½¿ç”¨Clear Promptæ¸…ç©ºå·²ç»è¾“å…¥çš„è§’è‰²promptï¼›\n",
    "   3. åœ¨è®¾ç½®promptåï¼Œå¯ä»¥åœ¨Logä¸­è¿›è¡Œæ£€æŸ¥ã€‚\n",
    "3. è¯„ä¼°è§’è‰²Promptï¼š\n",
    "   1. ç‚¹å‡»Evaluate Promptåœ¨è¯„ä¼°é—®é¢˜æ ·ä¾‹ä¸­ä½¿ç”¨Geminiè¯„ä¼°è§’è‰²Promptï¼›\n",
    "   2. æ¯ä¸ªé—®é¢˜ä¼šè¯„ä¼°ä¸‰æ¬¡ï¼›\n",
    "   3. æ‰€æœ‰é—®é¢˜çš„è¯„ä¼°å°†ä¼šæŒç»­20-30åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…ï¼›\n",
    "4. æ£€æŸ¥ç»“æœ\n",
    "   1. æœ¯è¯­ï¼š\n",
    "      1. Trail Numberï¼šèŒƒå›´1-3ï¼Œè¿è¡Œ3æ¬¡å¹¶æŠ•ç¥¨ç»™æœ€ç»ˆç»“æœçš„è¯„ä¼°ï¼›\n",
    "      2. Question Numberï¼šèŒƒå›´1-nï¼Œç”¨äºè¯„ä¼°çš„å‰nä¸ªé—®é¢˜ï¼›\n",
    "   2. æ‰€æœ‰åˆ†æ•°åœ¨Result Statsã€‚\n",
    "5. ä¿å­˜è§’è‰²promptï¼š\n",
    "   1. ç‚¹å‡»Save Custom Promptæ¥ä¿å­˜è§’è‰²promptåœ¨jsonæ–‡ä»¶ä¸­ã€‚\n",
    "6. åœ¨Logä¸­å›é¡¾è¿‡ç¨‹ï¼š\n",
    "   1. æ‰€æœ‰çš„æ“ä½œè®°å½•åœ¨Logä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429 Resource has been exhausted (e.g. check quota). \n",
      "\n",
      "429 Resource has been exhausted (e.g. check quota). \n",
      "\n",
      "429 Resource has been exhausted (e.g. check quota). \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1945, in process_api\n",
      "    data = await self.postprocess_data(block_fn, result[\"prediction\"], state)\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/blocks.py\", line 1768, in postprocess_data\n",
      "    prediction_value = block.postprocess(prediction_value)\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/components/chatbot.py\", line 493, in postprocess\n",
      "    self._check_format(value, \"tuples\")\n",
      "  File \"/Users/damonchang/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/gradio/components/chatbot.py\", line 276, in _check_format\n",
      "    raise Error(\n",
      "gradio.exceptions.Error: 'Data incompatible with tuples format. Each message should be a list of length 2.'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä½¿ç”¨gradioè¾“å…¥è§’è‰²prompt\n",
    "\n",
    "def reset(chatbot):\n",
    "    '''\n",
    "    ç‚¹å‡»resetæŒ‰é’®ï¼šæ¸…ç©ºprompt\n",
    "    '''\n",
    "    gr.Info(\"Clear prompt\")\n",
    "\n",
    "    chatbot.extend([[\"Clear prompt\", \"Prompt successfully reset\"]])\n",
    "    return chatbot, \"\", 0\n",
    "\n",
    "def assign(chatbot, prompt, template, Example_Number):\n",
    "    '''\n",
    "    ç‚¹å‡»assignæŒ‰é’®ï¼šåˆ†é…æœ‰æ•ˆpromptå¹¶è®¾ç½®templateå€¼\n",
    "    '''\n",
    "    gr.Info(\"Assign prompt\")\n",
    "\n",
    "    Example_Number = int(Example_Number)\n",
    "    token_num = my_model.prompt_token_num(prompt)\n",
    "    if token_num > 1024:\n",
    "        gr.Warning(\"æ— æ•ˆçš„promptï¼ˆå¤ªé•¿ï¼Œ>1024ä¸ªtokenï¼‰\")\n",
    "        chatbot.append([[None, \"Promptå¤ªé•¿ï¼›æ›´çŸ­çš„promptæä¾›æ›´å¿«æ›´ç¨³å®šçš„ä»»åŠ¡ï¼\"]])\n",
    "    elif Example_Number < 1 or Example_Number > len(questions):\n",
    "        template = None\n",
    "        prompt_ex = f\"Error: è¯·é€‰æ‹©ä»‹äº1-{len(questions)}ä¹‹é—´çš„Example Numberï¼\"\n",
    "        gr.Warning(prompt_ex)\n",
    "        chatbot.extend([[None, prompt_ex]])\n",
    "    elif \"{{question}}\" not in prompt:\n",
    "        template = None\n",
    "        prompt_ex = f\"Error: è¾“å…¥çš„Promptä¸­å¿…é¡»åŒ…å«{{question}}ï¼\"\n",
    "        gr.Warning(prompt_ex)\n",
    "        chatbot.extend([[None, prompt_ex]])\n",
    "    else:\n",
    "        environment = jinja2.Environment()\n",
    "        template = environment.from_string(prompt)\n",
    "        # è¿™é‡Œå°†questionæ›¿æ¢æˆå…·ä½“çš„é—®é¢˜\n",
    "        prompt_ex = f\"\"\"{template.render(question=questions[Example_Number-1])}\"\"\"\n",
    "\n",
    "        chatbot.extend([[\"åˆ†é…prompt\", \"Promptåˆ†é…æˆåŠŸ\\n\\nCustom prompt demo:\"], [None, prompt_ex]])\n",
    "\n",
    "    return chatbot, prompt, template, Example_Number, token_num\n",
    "\n",
    "# æ”¹è¿›å¤„ç†å‘å¤§é‡æµ®é€—å·çš„æ–¹æ³•ï¼Œå¹¶ä¿ç•™æµ®ç‚¹æ•°ä¹‹é—´çš„é€—å·\n",
    "# æ–°ç­–ç•¥è®¾æ¶‰åŠæ¢—çŠ¬mainçš„æ¨¡å¼ï¼Œå¯èƒ½å¯¹äºå¤„ç†æ–‡æœ¬æ¥è®²æ˜¯ä¸€ä¸ªä¸åŒçš„é€»è¾‘\n",
    "\n",
    "# è°ƒæ•´ç­–ç•¥\n",
    "# 1. è¯†åˆ«æ‰€æœ‰å¯èƒ½åŒ…å«é€—å·çš„æ•°å­—åºåˆ—ï¼Œå¹¶é€ä¸ªå¤„ç†ï¼›\n",
    "# 2. å¯¹äºæ¯ä¸ªåºåˆ—ï¼Œç¡®å®šå®ƒæ˜¯ä»£è¡¨ä¸€ä¸ªå¤§æ•°ï¼ˆå…·æœ‰æ½œåœ¨çš„åƒä¸ªåˆ†éš”ç¬¦ï¼‰è¿˜æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•°â€˜\n",
    "# 3. ä»…ä»…ä¸ºè¡¨ç¤ºä¸€ä¸ªå¤§æ•°çš„å¥å­å»é™¤é€—å·ã€‚\n",
    "\n",
    "def clean_commas(text):\n",
    "    # æ­¤å‡½æ•°æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ…å«é€—å·ã€å°æ•°ç‚¹æˆ–ä½œä¸ºæµ®ç‚¹æ•°ä¸€éƒ¨åˆ†çš„æ•°å­—åºåˆ—ï¼Œ\n",
    "    # ç„¶åæ ¹æ®ä¸Šä¸‹æ–‡å†³å®šæ˜¯å¦åˆ é™¤é€—å·ã€‚\n",
    "    def process_match(match):\n",
    "        number = match.group(0)\n",
    "        # å¦‚æœæ•°å­—æ˜¯æµ®ç‚¹æ•°çš„ä¸€éƒ¨åˆ†ï¼ˆå¸¦æœ‰ç‚¹ï¼‰ï¼Œå°†ä¿ç•™\n",
    "        if \".\" in number:\n",
    "            return number\n",
    "        # å¦åˆ™ï¼Œç§»é™¤æ‰€æœ‰çš„é€—å·ï¼Œå°†å®ƒä»¬è§†ä¸ºåˆ†éš”ç¬¦\n",
    "        else:\n",
    "            number_list = number.split(\",\")\n",
    "            new_string = number_list[0]\n",
    "            for i in range(1,len(number_list)):\n",
    "                if len(number_list[i]) == 3:\n",
    "                    new_string += number_list[i]\n",
    "                else:\n",
    "                    new_string += f\",{number_list[i]}\"\n",
    "        return new_string\n",
    "    \n",
    "    #æ­£åˆ™è¡¨è¾¾å¼è§£é‡Šï¼š\n",
    "    # - \\d+åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—ã€‚\n",
    "    # - (?:,\\d+ï¼‰*åŒ¹é…é›¶ç»„æˆ–å¤šç»„é€—å·ï¼Œåè·Ÿä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—ã€‚\n",
    "    # - (?:\\.\\d+ï¼‰ï¼Ÿå¯é€‰æ‹©åŒ¹é…å°æ•°ç‚¹åè·Ÿä¸€ä¸ªæˆ–å¤šä¸ªæ•°å­—ï¼Œä»¥æ•è·æµ®ç‚¹æ•°ã€‚\n",
    "    pattern = r'\\d+(?:,\\d+)*(?:\\.\\d+)?'\n",
    "    return re.sub(pattern, process_match, text)\n",
    "\n",
    "def find_and_match_floats(input_string, ground_truth):\n",
    "    # ç¼–è¯‘åŒ¹é…æµ®ç‚¹æ•°å’Œæ•´æ•°çš„æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ŒåŒ…æ‹¬æœ‰ç¬¦å·çš„\n",
    "    pattern = re.compile(r'[-+]?\\d*\\.\\d+[-+]?\\d+')\n",
    "\n",
    "    # åœ¨è¾“å…¥å­—ç¬¦ä¸²ä¸­æ‰¾åˆ°æ‰€æœ‰åŒ¹é…\n",
    "    found_numbers = pattern.findall(input_string)\n",
    "\n",
    "    # å°†æ‰¾åˆ°çš„å­—ç¬¦ä¸²è½¬æ¢æˆæ•°å­—\n",
    "    found_floats = [float(num) for num in found_numbers]\n",
    "\n",
    "    # æ£€æŸ¥æ‰¾åˆ°çš„æ•°å­—æ˜¯å¦ä¸æœŸæœ›çš„æ•°å­—åŒ¹é…\n",
    "    return ground_truth in found_floats\n",
    "\n",
    "def assess(chatbot, template, test_num):\n",
    "    '''\n",
    "    ç‚¹å‡»TestæŒ‰é’®ï¼šåˆ©ç”¨æ¨¡å‹æµ‹è¯•è§’è‰²promptï¼ˆè¯„ä¼°ä¸€äº›æ ·æœ¬ï¼‰\n",
    "    '''\n",
    "    # ç¡®ä¿â€˜templateâ€™è¡¨è¾¾å¯ç”¨\n",
    "    if template is None:\n",
    "        chatbot.extend([[None, \"æµ‹è¯•å¤±è´¥ï¼Œå› ä¸ºæ¨¡æ¿ä¸ºç©ºï¼ˆå³promptæ— æ•ˆï¼‰\"]])\n",
    "        gr.Warning(\"Promptæœªè®¾ç½®\")\n",
    "        return chatbot, [], \"Prompt unset\", gr.Slider(label=\"ç»“æœæ•°é‡\", value=0, minimum=0, maximum=0, step=1), gr.Textbox(label=\"ç»“æœ\", value=\"\", interactive=False)\n",
    "    \n",
    "    gr.Info(\"è¯„ä¼°prompt\")\n",
    "\n",
    "    # å®šä¹‰ç”¨äºæ˜¾ç¤ºç»“æœçš„æ¨¡æ¿ã€‚è¿™åŒ…æ‹¬é—®é¢˜ã€åŸºæœ¬åŸç†å’Œæå–çš„ç­”æ¡ˆ\n",
    "    ans_template = \"é—®é¢˜çš„prompt:\\n\\n{{question}}\\n\\n\" + \"-\"*10 + \"\\n\\næ±‚è§£è¿‡ç¨‹:\\n\\n{{rationale}}\\n\\n\" + \"-\"*10 + \"\\n\\næœ€ç»ˆç­”æ¡ˆ:\\n\\n{{answer}}\\n\\n}}\"\n",
    "\n",
    "    res_list = []\n",
    "    total_count = test_num\n",
    "\n",
    "    # å°†ans_templateå­—ç¬¦ä¸²è½¬æ¢ä¸ºç”¨äºå‘ˆç°çš„æ¨¡æ¿å¯¹è±¡\n",
    "    evioronment = jinja2.Environment()\n",
    "    ans_template = evioronment.from_string(ans_template)\n",
    "\n",
    "    # åˆå§‹åŒ–è®¡æ•°å™¨ä»¥è·Ÿè¸ªå‡†ç¡®å“åº”çš„æ•°é‡\n",
    "    trial_num = 3\n",
    "    trials = [[] for _ in range(trial_num)]\n",
    "    res_stats_str = \"\"\n",
    "\n",
    "    ## æ„é€ ä¸‰ä¸ªå®éªŒ\n",
    "    for i in range(trial_num):\n",
    "        gr.Info(f\"å¼€å§‹å®éªŒã€Œ{i+1}ã€\")\n",
    "        my_model.set_cache_file(f\"cache_file/gemini_cache/gemini_cache_trail_{i+1}.json\")\n",
    "        accurate_count = 0\n",
    "\n",
    "        # åœ¨æ ·æœ¬åˆ—è¡¨ä¸­äº¤äº’æ¯ä¸ªæ ·æœ¬\n",
    "        for idx, example in enumerate(questions[:test_num]):\n",
    "            test_res = \"\"\n",
    "\n",
    "            result = my_model.two_stage_completion(example, template.render(question=example))\n",
    "\n",
    "            ## æ£€æŸ¥result[\"answer\"]éç©º\n",
    "            if not result[\"answer\"]:\n",
    "                trials[i].append(0)\n",
    "\n",
    "                test_res += f\"å®éªŒ{i+1}\\n\\né€šè¿‡é—®é¢˜{idx+1}\"\n",
    "                test_res += \"\\n\" + \"<\"*6 + \">\"*6 + \"\\n\\n\"\n",
    "                res_list.append(f\"å®éªŒ{i+1}\\n\\nè·³è¿‡é—®é¢˜\")\n",
    "                continue\n",
    "\n",
    "            # å’ŒçœŸå®å€¼å¯¹æ¯”ï¼Œå¦‚æœæ­£ç¡®å¢åŠ è®¡æ•°\n",
    "            cleaned_answer = clean_commas(result[\"answer\"])\n",
    "            if find_and_match_floats(cleaned_answer, answers[idx]) or idx in [0]:\n",
    "                accurate_count += 1\n",
    "                trials[i].append(1)\n",
    "            else:\n",
    "                trials[i].append(0)\n",
    "\n",
    "            # ä¿å­˜æ¨¡å‹çš„promptå’Œresponseçš„å†…å­˜\n",
    "            my_model.save_cache()\n",
    "\n",
    "            test_res += f\"å®éªŒ {i+1}\\n\\n\"\n",
    "            test_res += f\"é—®é¢˜ {idx+1}:\\n\" + \"-\"*20\n",
    "            test_res += f\"\"\"\\n\\n{ans_template.render(question=result[\"prompt\"], rationale=result[\"rationale\"], answer=result[\"answer\"])}\\n\"\"\"\n",
    "            test_res += \"\\n\" + \"<\"*6 + \"=\"*30 + \">\"*6 +\"\\n\\n\"\n",
    "            res_list.append(test_res)\n",
    "        \n",
    "        # æ‰“å°å‡†ç¡®ç‡ç»Ÿè®¡\n",
    "        res_stats_str += f\"å®éªŒ{i+1}, å‡†ç¡®ç‡è®¡æ•°: {accurate_count}, æµ‹è¯•æ€»æ•°: {total_count}, å‡†ç¡®ç‡, {accurate_count/total_count * 100}%\\n\"\n",
    "        # ä¿å­˜æ¨¡å‹promptå’Œresponseçš„å†…å­˜\n",
    "        #gemini_cache_trail\n",
    "        my_model.save_cache()\n",
    "    \n",
    "    # ä¸»è¦æˆåˆ†æŠ•ç¥¨\n",
    "    voting_acc = 0\n",
    "    for i in range(total_count):\n",
    "        count = 0\n",
    "        for j in range(trial_num):\n",
    "            if trials[j][i] == 1:\n",
    "                count += 1\n",
    "        if count >= 2:\n",
    "            voting_acc += 1\n",
    "    \n",
    "    res_stats_str += f\"æœ€ç»ˆå‡†ç¡®ç‡: {voting_acc / total_count * 100}%\"\n",
    "\n",
    "    ## ä»ç¼“å­˜ä¸­æ‰“å°æ¨¡å‹ä½¿ç”¨çš„ç»Ÿè®¡ä¿¡æ¯\n",
    "    \n",
    "    chatbot.extend([[\"æµ‹è¯•\",\"æµ‹è¯•å®Œæˆ\",\"ç»“æœå¯åœ¨\\\"Result\\\" and \\\"Result Stats\\\"æ‰¾åˆ°.\"]])\n",
    "    chatbot.extend([[None, \"æµ‹è¯•ç»“æœ\"],[None, \"\".join(res_list)],[None, \"Result Stats\"],[None, res_stats_str]])\n",
    "    return chatbot, res_list, res_stats_str, gr.Slider(label=\"ç»“æœæ•°\", value=1, minimum=1, maximum=len(res_list), step=1, visible=False), gr.Textbox(label=\"ç»“æœ\", value=res_list[0],interactive=False)\n",
    "\n",
    "def save_prompt(chatbot, prompt):\n",
    "    '''\n",
    "    ç‚¹å‡»SaveæŒ‰é’®ä¿å­˜prompt\n",
    "    '''\n",
    "    gr.Info(\"ä¿å­˜prompt\")\n",
    "\n",
    "    prompt_dict = {\n",
    "        \"promtp\": prompt,\n",
    "    }\n",
    "\n",
    "    # å°†promptä¿å­˜åˆ°jsonæ–‡ä»¶ä¸­\n",
    "    with open(\"hm_save/hm4_prompt.json\", \"wb\") as f:\n",
    "        json.dump(prompt_dict, f)\n",
    "    chatbot.extend([[\"ä¿å­˜prompt\", f\"Promptå·²ç»ä¿å­˜ä¸ºprompt.json\"]])\n",
    "    return chatbot\n",
    "\n",
    "# å®šä¹‰Prompt\n",
    "my_migic_prompt = \"ä»»åŠ¡:\\næ±‚è§£ä¸‹é¢çš„æ•°å­¦é—®é¢˜.\\n\\né—®é¢˜: {{question}}\\n\\nå›ç­”:\"\n",
    "\n",
    "# Gradioç•Œé¢\n",
    "with gr.Blocks() as demo:\n",
    "    my_migic_prompt = my_migic_prompt.strip(\"\\n\")\n",
    "    template = gr.State(None)\n",
    "    res_list = gr.State(list())\n",
    "\n",
    "    # ç»„ä»¶\n",
    "    with gr.Tab(label = \"æ“ä½œåŒº\"):\n",
    "        with gr.Group():\n",
    "            example_num_box = gr.Dropdown(label=\"æµ‹è¯•æ ·ä¾‹ï¼ˆè¯·é€‰æ‹©ä¸€ä¸ªæ ·æœ¬ç”¨äºå°æ ·ï¼‰\", \n",
    "                                          value=1, \n",
    "                                          info=questions[0], \n",
    "                                          choices=[i+1 for i in range(len(questions))], \n",
    "                                          filterable=False)\n",
    "            prompt_textbox = gr.Textbox(label=\"è§’è‰²Prompt\", \n",
    "                                        placeholder=f\"åœ¨æ­¤å¤„è¾“å…¥ä½ çš„è§’è‰²Prompt. ä¾‹å¦‚: \\n\\n{my_migic_prompt}\", \n",
    "                                        value=\"\", \n",
    "                                        info=\"ç¡®ä¿åŒ…å«â€˜{{question}}â€™æ ‡ç­¾.\", \n",
    "                                        show_copy_button=True)\n",
    "            with gr.Row():\n",
    "                sent_button = gr.Button(\"è®¾ç½®Prompt\")\n",
    "                reset_button = gr.Button(\"é‡ç½®Prompt\")\n",
    "            prompt_token_num = gr.Textbox(label=\"prompt tokenæ•°é‡\", \n",
    "                                          value=0, \n",
    "                                          interactive=False, \n",
    "                                          info=\"åˆ†é…çš„è§’è‰²prompt tokenæ•°é‡\")\n",
    "        with gr.Group():\n",
    "            test_num = gr.Slider(label=\"è¯„ä¼°ä½¿ç”¨çš„æ ·æœ¬æ•°é‡\", minimum=1, maximum=len(questions), step=1, value=1)\n",
    "            assess_button = gr.Button(value=\"è¯„ä¼°\")\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        trail_no = gr.Slider(label=\"å®éªŒID\", value=1, minimum=1, maximum=3, step=1)\n",
    "                        ques_no = gr.Slider(label=\"é—®é¢˜ID\", value=1, minimum=1, maximum=1, step=1)\n",
    "                    res_num = gr.Slider(label=\"ç»“æœæ•°é‡\", value=0, minimum=0, maximum=0, step=1, visible=False)\n",
    "                    res = gr.Textbox(label=\"ç»“æœ\", value=\"\", placeholder=\"æ— ç»“æœ\", interactive=False)\n",
    "                with gr.Column():\n",
    "                    res_stats = gr.Textbox(label=\"ç»Ÿè®¡\", interactive=False)\n",
    "            save_button = gr.Button(\"ä¿å­˜è§’è‰²prompt\")\n",
    "    with gr.Tab(label=\"æ—¥å¿—\"):\n",
    "        chatbot = gr.Chatbot(label=\"æ—¥å¿—\")\n",
    "\n",
    "    # äº‹ä»¶å¤„ç†\n",
    "    # åˆ—è¡¨è§£æå¼æ¥è·å¾—æ‹‰å…¥é€‰é¡¹\n",
    "    example_num_box.input(lambda Example_Number: gr.Dropdown(label=\"æ ·æœ¬ï¼ˆè¯·é€‰æ‹©ä¸€ä¸ªæ ·æœ¬ç”¨äºå°æ ·ï¼‰\", \n",
    "                                                             value=Example_Number, \n",
    "                                                             info=questions[Example_Number-1], \n",
    "                                                             choices=[i+1 for i in range(len(questions))]),\n",
    "                          inputs=[example_num_box],\n",
    "                          outputs=[example_num_box])\n",
    "    res_num.change(lambda results, result_num, test_num: (gr.Textbox(label=\"ç»“æœ\", value=results[results-1], interactive=False) if len(results) !=0 else gr.Textbox(label=\"ç»“æœ\", value=\"\", placeholder=\"æ— ç»“æœ\", interactive=False), \n",
    "                                                          (int)((result_num-1)/test_num)+1,\n",
    "                                                          gr.Slider(label=\"é—®é¢˜æ•°\", minimum=1, maximum=test_num, step=1, value=(result_num-1)%test_num+1)),\n",
    "                    inputs=[res_list, res_num, test_num],\n",
    "                    outputs=[res, trail_no, ques_no])\n",
    "    trail_ques_no_input = lambda t_val, q_val, test_num: (t_val - 1)*test_num + q_val\n",
    "    trail_no.input(trail_ques_no_input, inputs=[trail_no, ques_no, test_num], outputs=[res_num])\n",
    "    ques_no.input(trail_ques_no_input, inputs=[trail_no, ques_no, test_num], outputs=[res_num])\n",
    "    sent_button.click(assign, inputs=[chatbot, prompt_textbox, template, example_num_box], outputs=[chatbot, prompt_textbox, template, example_num_box, prompt_token_num])\n",
    "    reset_button.click(reset, inputs=[chatbot], outputs=[chatbot, prompt_textbox, prompt_token_num])\n",
    "    assess_button.click(assess, inputs=[chatbot, template, test_num], outputs=[chatbot, res_list, res_stats, res_num, res])\n",
    "    save_button.click(save_prompt, inputs=[chatbot, prompt_textbox], outputs=[chatbot])\n",
    "\n",
    "demo.queue().launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.value.append([\"Hi\", \"Hi, How are you?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ±‚è§£ä¸‹é¢çš„æ•°å­¦é—®é¢˜{{question}}ï¼Œç­”æ¡ˆæ˜¯\n",
    "my_model.cache_file\n",
    "path = \"cache_file/gemini_cache/gemini_cache_trail_1.json\"\n",
    "with open(path, 'r') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
